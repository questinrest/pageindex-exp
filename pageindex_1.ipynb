{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb803f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanm\\Desktop\\learning\\pageindex-exp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import asyncio\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import pageindex.utils as utils \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b86329",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RLM_tree.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tree_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f098c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(prompt: str, model: str, reasoning: bool, temperature: float = 0.0):\n",
    "    if reasoning:\n",
    "        client = openai.AsyncOpenAI(\n",
    "            api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "            base_url=\"https://openrouter.ai/api/v1\"\n",
    "        )\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "    else:\n",
    "        groq_llm = ChatGroq(\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "            model_name=model,   # \"openai/gpt-oss-120b\" for for generating the answer\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        response = await groq_llm.ainvoke(\n",
    "            [HumanMessage(content=prompt)]\n",
    "        )\n",
    "\n",
    "        return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9303e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map = utils.create_node_mapping(tree_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29806a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask(query: str) -> str:\n",
    "    tree_without_text = utils.remove_fields(node_map, fields=['text'])\n",
    "\n",
    "    search_prompt = f\"\"\"\n",
    "You are given a question and a tree structure of a document.\n",
    "Each node contains a node id, node title, and a corresponding summary.\n",
    "Your task is to find all nodes that are likely to contain the answer to the question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Document tree structure:\n",
    "{json.dumps(tree_without_text, indent=2)}\n",
    "\n",
    "Please reply in the following JSON format:\n",
    "{{\n",
    "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
    "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
    "}}\n",
    "Directly return the final JSON structure. Do not output anything else.\n",
    "\"\"\"\n",
    "    \"stepfun/step-3.5-flash:free\"\n",
    "    tree_search_result = await call_llm(search_prompt, reasoning=True, model=\"stepfun/step-3.5-flash:free\")\n",
    "\n",
    "    try:\n",
    "        tree_search_result_json = json.loads(tree_search_result)\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Sorry, I couldn't process that query. Please try again.\"\n",
    "    \n",
    "    print(\"\\nReasoning Process:\")\n",
    "    utils.print_wrapped(tree_search_result_json.get(\"thinking\", \"\"))\n",
    "\n",
    "    # Print retrieved nodes\n",
    "    node_list = tree_search_result_json.get(\"node_list\", [])\n",
    "    print(\"\\nRetrieved Nodes:\")\n",
    "    for node_id in node_list:\n",
    "        node = node_map.get(node_id)\n",
    "        if node:\n",
    "            page = node.get(\"page_index\", f\"{node.get('start_index', '?')}-{node.get('end_index', '?')}\")\n",
    "            print(f\"  Node ID: {node['node_id']}\\t Page: {page}\\t Title: {node['title']}\")\n",
    "\n",
    "    # Extract relevant context\n",
    "    relevant_content = \"\\n\\n\".join(\n",
    "        node_map[nid][\"text\"] for nid in node_list if nid in node_map and \"text\" in node_map[nid]\n",
    "    )\n",
    "\n",
    "    if not relevant_content:\n",
    "        return \"No relevant sections found in the document.\"\n",
    "\n",
    "    print(\"\\nRetrieved Context (preview):\")\n",
    "    #utils.print_wrapped(relevant_content[:1000] + \"...\")\n",
    "\n",
    "    answer_prompt = f\"\"\"\n",
    "Answer the question based on the context:\n",
    "\n",
    "Question: {query}\n",
    "Context: {relevant_content}\n",
    "\n",
    "Provide a clear, concise answer based only on the context provided.\n",
    "\"\"\"\n",
    "    answer = await call_llm(answer_prompt, model=\"llama-3.3-70b-versatile\", reasoning=False)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba733e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  PageIndex Document Chatbot\")\n",
    "    print(\"  Ask any question about the document.\")\n",
    "    print('  Type \"exit\" to quit.')\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    while True:\n",
    "        print()\n",
    "        query = input(\"You: \").strip()\n",
    "        if not query:\n",
    "            continue\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        answer = await ask(query)\n",
    "        print(f\"\\nBot: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a907d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  PageIndex Document Chatbot\n",
      "  Ask any question about the document.\n",
      "  Type \"exit\" to quit.\n",
      "============================================================\n",
      "\n",
      "\n",
      "Reasoning Process:\n",
      "The question asks about the problem Recursive Language Models (RLMs) aim to solve and how they\n",
      "differ from context compaction approaches. Based on the document summaries: Node 0000 (Abstract)\n",
      "introduces RLMs as a paradigm for processing arbitrarily long prompts, highlighting their ability to\n",
      "exceed standard context windows and outperform existing methods. Node 0001 (Introduction) explicitly\n",
      "states the problem: limitations of LLMs with small context windows and context rot, and contrasts\n",
      "RLMs with context condensation and prior agent architectures. Node 0002 (Recursive Language Models)\n",
      "explains RLMs' approach of treating prompts as an external environment and contrasts them with\n",
      "scaffolds that copy prompts into context or generate output directly, both limited by context size.\n",
      "Node 0007 (Related Works) categorizes long-context approaches, mentions lossy context management\n",
      "(likely context compaction), and distinguishes RLMs by their implicit context management. These\n",
      "nodes directly address both parts of the question, while other nodes focus on tasks, results, or\n",
      "specifics not central to the core problem and differences.\n",
      "\n",
      "Retrieved Nodes:\n",
      "  Node ID: 0000\t Page: 1-1\t Title: Abstract\n",
      "  Node ID: 0001\t Page: 1-3\t Title: 1. Introduction\n",
      "  Node ID: 0002\t Page: 3-4\t Title: 2. Recursive Language Models\n",
      "  Node ID: 0007\t Page: 7-8\t Title: 5 Related Works\n",
      "\n",
      "Retrieved Context (preview):\n",
      "\n",
      "Bot: Recursive Language Models (RLMs) aim to solve the problem of processing arbitrarily long prompts, which is a limitation of large language models (LLMs) due to their limited context windows. RLMs differ from context compaction approaches in that they treat the prompt as part of an external environment and allow the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt, rather than summarizing or truncating the input context. This approach enables RLMs to process inputs up to two orders of magnitude beyond model context windows and outperform other approaches, including context compaction, in terms of quality and cost.\n",
      "\n",
      "\n",
      "Reasoning Process:\n",
      "The question asks for differences between Algorithm 1 (RLM) and Algorithm 2 in handling long inputs,\n",
      "recursion, and output generation. From the document, Algorithm 1 is clearly Recursive Language\n",
      "Models (RLMs), and Algorithm 2 is likely a baseline such as CodeAct or Summary agent, based on\n",
      "comparisons in the text. Nodes that directly discuss or contrast RLMs with other methods in these\n",
      "aspects are relevant. Key nodes include: 0002 (introduces RLMs and contrasts with less expressive\n",
      "scaffolds regarding recursion and long input handling), 0004 (details comparative study with\n",
      "baselines like CodeAct), 0006 (describes emergent RLM patterns for recursion and output generation),\n",
      "0007 (related works contrasting RLMs with other approaches), and 0013 (provides prompts for RLMs and\n",
      "CodeAct agents, highlighting implementation differences). These nodes cover the conceptual and\n",
      "practical differences in the specified areas.\n",
      "\n",
      "Retrieved Nodes:\n",
      "  Node ID: 0002\t Page: 3-4\t Title: 2. Recursive Language Models\n",
      "  Node ID: 0004\t Page: 4-5\t Title: 3.2 Methods and Baselines\n",
      "  Node ID: 0006\t Page: 7-7\t Title: 4.1 Emergent Patterns in RLM Trajectories\n",
      "  Node ID: 0007\t Page: 7-8\t Title: 5 Related Works\n",
      "  Node ID: 0013\t Page: 15-20\t Title: C.1 Prompts for Experiments\n",
      "\n",
      "Retrieved Context (preview):\n",
      "\n",
      "Bot: Algorithm 1 (RLM) differs from Algorithm 2 in three key ways:\n",
      "\n",
      "1. **Handling long inputs**: RLM treats the user prompt as part of the environment, allowing it to process the prompt without copying it into the root context window. In contrast, Algorithm 2 puts the user prompt into the LLM context window, inheriting the window limitations of the LLM.\n",
      "\n",
      "2. **Recursion**: RLM requires symbolic recursion, where code running inside the environment can invoke the LLM on programmatically constructed transformations of the prompt. Algorithm 2, on the other hand, cannot invoke the sub-LLM programmatically and can only delegate a few explicitly verbalized tasks.\n",
      "\n",
      "3. **Output generation**: RLM generates output by returning variables in the REPL as output, allowing it to produce essentially unbounded tokens. Algorithm 2, by contrast, asks the LLM to autoregressively generate the output directly, limiting the output length to the context window of the LLM.\n",
      "\n",
      "\n",
      "Reasoning Process:\n",
      "The question asks for experimental results on why RLMs outperform base models specifically on\n",
      "OOLONG-Pairs and the role of recursive sub-calling. From the document summaries, node 0005 (4.\n",
      "Results and Discussion) is the primary results section, stating that RLMs demonstrate strong\n",
      "performance on information-dense tasks like OOLONG-Pairs due to recursive sub-calling, and that base\n",
      "model performance degrades with complexity while RLMs scale effectively. Node 0006 (4.1 Emergent\n",
      "Patterns in RLM Trajectories) elaborates on how recursive sub-calling is implemented, detailing\n",
      "strategies like chunking and recursive LM calls to handle long contexts. Other nodes define OOLONG-\n",
      "Pairs (e.g., 0003, 0014) or provide examples (e.g., 0023), but do not present general experimental\n",
      "results or explain the outperformance mechanism. Thus, nodes 0005 and 0006 are most relevant.\n",
      "\n",
      "Retrieved Nodes:\n",
      "  Node ID: 0005\t Page: 5-7\t Title: 4. Results and Discussion\n",
      "  Node ID: 0006\t Page: 7-7\t Title: 4.1 Emergent Patterns in RLM Trajectories\n",
      "\n",
      "Retrieved Context (preview):\n",
      "\n",
      "Bot: According to the experimental results, RLMs outperform base models on OOLONG-Pairs because they can handle extremely information-dense tasks through recursive sub-calling. The recursive sub-calling of RLMs provides strong benefits on information-dense inputs by allowing the model to break down the task into smaller, more manageable parts, and then combine the results to form a final answer. This is evident in the significant improvement in F1 scores achieved by RLMs using GPT-5 and Qwen3-Coder models on OOLONG-Pairs, with scores of 58.0% and 23.1% respectively, compared to the base models which made little progress with F1 scores of < 0.1%.\n",
      "\n",
      "\n",
      "Reasoning Process:\n",
      "The question asks for trade-offs between base language models and RLMs in performance, cost, and\n",
      "input length. I reviewed the document summaries to find nodes that explicitly compare these aspects.\n",
      "The Abstract (0000) and Introduction (0001) introduce RLMs and hint at superior performance and\n",
      "comparable costs for long contexts. Section 2 (0002) explains how RLMs handle inputs beyond base\n",
      "model context windows, addressing input length trade-offs. Section 3.2 (0004) details a comparative\n",
      "study with performance tables and API costs. Section 4 (0005) discusses results showing RLMs\n",
      "outperform base LMs on long-context tasks with comparable or lower costs. Task 20 (0020) mentions\n",
      "scaling performance and cost comparisons, noting RLMs maintain performance with increasing input\n",
      "length cheaper than base models. Section F (0026) provides in-depth runtime and cost analysis,\n",
      "including histograms for base models like GPT-5. These nodes collectively cover performance, cost,\n",
      "and input length trade-offs.\n",
      "\n",
      "Retrieved Nodes:\n",
      "  Node ID: 0000\t Page: 1-1\t Title: Abstract\n",
      "  Node ID: 0001\t Page: 1-3\t Title: 1. Introduction\n",
      "  Node ID: 0002\t Page: 3-4\t Title: 2. Recursive Language Models\n",
      "  Node ID: 0004\t Page: 4-5\t Title: 3.2 Methods and Baselines\n",
      "  Node ID: 0005\t Page: 5-7\t Title: 4. Results and Discussion\n",
      "  Node ID: 0020\t Page: 23-24\t Title: Task 20\n",
      "  Node ID: 0026\t Page: 34-38\t Title: F. Additional Runtime and Cost Analysis of RLMs\n",
      "\n",
      "Retrieved Context (preview):\n",
      "\n",
      "Bot: The trade-offs between using a base language model and using a Recursive Language Model (RLM) are as follows:\n",
      "\n",
      "1. **Performance**: RLMs can process inputs up to two orders of magnitude beyond the model context windows and outperform base language models on long-context tasks, even for shorter prompts.\n",
      "2. **Cost**: RLMs have comparable or lower costs than base language models, with an average cost of $0.99 compared to $1.50-$2.75 for GPT-5-mini on the BrowseComp-Plus task. However, RLMs can have high variance in costs due to differences in trajectory lengths.\n",
      "3. **Input Length**: RLMs can handle arbitrarily long inputs, while base language models are limited by their context window size. RLMs can scale well beyond the context limit of the model, while base models degrade significantly as input length increases.\n",
      "\n",
      "Overall, RLMs offer improved performance and scalability for long-context tasks, with comparable or lower costs, but may require more careful management of trajectory lengths to optimize costs.\n",
      "\n",
      "\n",
      "Reasoning Process:\n",
      "The question focuses on how fine-tuning Qwen3-8B improves its performance as an RLM and what this\n",
      "suggests about training models for recursive reasoning. I scanned the document summaries for nodes\n",
      "that explicitly discuss RLM-Qwen3-8B, its fine-tuning process, performance improvements, or insights\n",
      "into training for recursive reasoning. Key nodes include: 0012, which details the training of RLM-\n",
      "Qwen3-8B; 0004, which introduces the fine-tuned model; 0000, which mentions substantial\n",
      "improvements; 0001, which notes rapid improvement with minimal post-training; 0005, which discusses\n",
      "training enhancing general performance; and 0008, which suggests training native RLMs as an\n",
      "advancement. These nodes collectively address both parts of the question by covering the fine-tuning\n",
      "methodology, observed improvements, and broader implications for model training.\n",
      "\n",
      "Retrieved Nodes:\n",
      "  Node ID: 0000\t Page: 1-1\t Title: Abstract\n",
      "  Node ID: 0001\t Page: 1-3\t Title: 1. Introduction\n",
      "  Node ID: 0004\t Page: 4-5\t Title: 3.2 Methods and Baselines\n",
      "  Node ID: 0005\t Page: 5-7\t Title: 4. Results and Discussion\n",
      "  Node ID: 0008\t Page: 8-8\t Title: 6. Limitations and Future Work\n",
      "  Node ID: 0012\t Page: 13-15\t Title: A. Additional Training Details\n",
      "\n",
      "Retrieved Context (preview):\n",
      "\n",
      "Bot: Fine-tuning Qwen3-8B as an RLM improves its performance by 28.3% on average across four long-context tasks. This suggests that training models specifically for recursive reasoning can lead to significant performance gains, even with limited additional training data (in this case, 1,000 samples). The fine-tuning process focuses on improving the root model's ability to interact with the programmatic representation of the prompt and to discern when sub-calls are useful, allowing the model to learn to operate effectively as an RLM.\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c92d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a087d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f8b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
