{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6873f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanm\\Desktop\\learning\\pageindex-exp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440aac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5eaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"C:\\Users\\amanm\\Desktop\\learning\\pageindex-exp\\RLM.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ce6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2cb25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanm\\AppData\\Local\\Temp\\ipykernel_5756\\1474760240.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 707.64it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5a9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06458ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_rag(question):\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer the question based on the context:\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "Provide a clear, concise answer based only on the context provided.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"num_chunks_used\": len(retrieved_docs),\n",
    "        \"source_pages\": [doc.metadata.get(\"page\", None) for doc in retrieved_docs]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ca01d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANSWER:\n",
      "\n",
      "Fine-tuning Qwen3-8B as an RLM improves its performance by 28.3% on average, suggesting that training models specifically for recursive reasoning can significantly enhance their ability to manipulate the REPL and launch recursive calls, even when applied to unrelated tasks. This implies that targeted fine-tuning can make models more effective in recursive reasoning tasks, leading to better decision-making and reduced inference costs.\n",
      "\n",
      "Chunks Used: 5\n",
      "Source Pages: [4, 13, 23, 0, 6]\n"
     ]
    }
   ],
   "source": [
    "question = \"How does fine-tuning Qwen3-8B improve its performance as an RLM, and what does this suggest about training models specifically for recursive reasoning?\"\n",
    "\n",
    "result = classical_rag(question)\n",
    "\n",
    "print(\"\\nANSWER:\\n\")\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\nChunks Used:\", result[\"num_chunks_used\"])\n",
    "print(\"Source Pages:\", result[\"source_pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc61f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
