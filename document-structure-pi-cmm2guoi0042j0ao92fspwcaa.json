[
  {
    "title": "ABSTRACT",
    "node_id": "0000",
    "summary": "The text introduces LEANN, a storage-efficient index for embedding-based vector search, addressing the high storage overhead of traditional indices that store embeddings and metadata. LEANN recomputes embeddings on the fly and compresses proximity graph indices, achieving significant storage reduction (up to 50x) while maintaining state-of-the-art accuracy and comparable latency for applications like RAG. It also supports efficient index construction and updates, making vector search more practical for personal devices and large datasets.",
    "start_index": 1,
    "end_index": 1,
    "text": "# ABSTRACT\n\nEmbedding-based vector search underpins many important applications, such as recommendation and retrieval-augmented generation (RAG). It relies on vector indices to enable efficient search. However, these indices require storing high-dimensional embeddings and large index metadata, whose total size can be several times larger than the original data (e.g., text chunks). Such high storage overhead makes it difficult, or even impractical, to deploy vector search on personal devices or large-scale datasets. To tackle this problem, we propose LEANN, a storage-efficient index for vector search that recomputes embeddings on the fly instead of storing them, and compresses state-of-the-art proximity graph indices while preserving search accuracy. LEANN delivers high-quality vector search while using only a fraction of the storage (e.g.,  $5\\%$  of the original data) and supporting storage-efficient index construction and updates. On real-world benchmarks, LEANN reduces index size by up to  $50\\times$  compared with conventional indices, while maintaining SOTA accuracy and comparable latency for RAG applications.\n"
  },
  {
    "title": "1 INTRODUCTION",
    "node_id": "0001",
    "summary": "The text introduces vector search as a core technology for applications like content search and question answering, where data is mapped to high-dimensional embeddings. It discusses the common use of Approximate Nearest Neighbor Search (ANNS) due to the computational cost of exact search, with recall being a key performance metric. A comparison of indexing methods (BM25, HNSW, PQ, LEANN) for Retrieval-Augmented Generation (RAG) is presented, highlighting that vector search methods like HNSW offer higher accuracy than keyword search. A significant challenge identified is the substantial storage overhead of embeddings and index metadata, particularly for local deployments on resource-constrained devices. The text proposes LEANN, a novel storage-efficient vector index designed to address this challenge. LEANN's approach involves recomputing embeddings at query time, using a two-level search with approximate distances to prune computations, and employing dynamic batching for efficiency. It also reduces index metadata size through high-degree preserving graph pruning. LEANN incorporates optimizations for index building and updates, demonstrating significant storage reduction while maintaining accuracy and reasonable latency, especially in RAG workloads where LLM generation time dominates. The background section further elaborates on vector search, ANNS, recall, and the mechanics of proximity graph indexes and their best-first search algorithms.",
    "start_index": 1,
    "end_index": 3,
    "text": "# 1 INTRODUCTION\n\nAdvances in foundation models have led to increasingly powerful embedding models, and embedding-based vector search has become a core functionality underpinning many important applications, such as content search (Lee et al., 2024; Yin et al., 2024), personal assistants (He et al., 2019; Cai et al., 2024), and question answering (Yang et al., 2018; Joshi et al., 2017). In particular, data objects with complex semantics (e.g., texts, images, videos) are mapped to high-dimensional vectors with an embedding model, so that semantically similar or related objects have a small distance between their embeddings. To retrieve objects from a database, the query object (e.g., a text description) is first embedded as a query vector and then used to search for the top- $k$  most similar vectors. Since exact vector search requires a linear scan in high-dimensional space, approximate nearest neighbor search (ANNS) is commonly adopted (Aumüller et al., 2020), which returns most rather than all of the top- $k$  neighbors. The result quality of ANNS is typically measured by recall, defined as the fraction of ground-truth top- $k$  neighbors that appear in the  $k$  retrieved vectors.\n\nTable 1 shows that when retrieval-augmented generation (RAG) is applied to a question answering (QA) dataset, vec\n\nTable 1. Storage overhead and runtime statistics of different indexing methods for RAG, evaluated on a 76 GB text datastore (Computer, 2023) and a QA dataset (Kwiatkowski et al., 2019) using the Qwen3-4B model on an RTX 4090.\n\n|  Metrics | BM25 | HNSW | PQ | LEANN  |\n| --- | --- | --- | --- | --- |\n|  Downstream accuracy (%) | 18.3 | 25.5 | 17.9 | 25.5  |\n|  Storage size (GB) | 59 | 188 | 20 | 4  |\n|  Index metadata | - | 15 | 15 | 2  |\n|  Vectors | - | 173 | 5 | 2  |\n|  End-to-end latency (s) | 21.36 | 20.95 | 25.45 | 23.34  |\n|  Search | 0.03 | 0.05 | 4.53 | 2.48  |\n|  Response generation | 21.33 | 20.90 | 20.92 | 20.86  |\n\ntor search methods such as Hierarchical Navigable Small World (HNSW) (Malkov &amp; Yashunin, 2018) yield substantially higher downstream accuracy than traditional keyword-based search approaches like BM25 (Craswell et al., 2021). This is because vector search better retrieves passages that are semantically related to the query intent.\n\nDeploying ANNS: Challenges and Opportunities. Vector search demands substantial storage, as high-dimensional embeddings and index metadata can be several times larger than the original data (Shao et al., 2024). Table 1 shows that a 76 GB text corpus requires 173 GB for embeddings and 15 GB for the HNSW index, a state-of-the-art graph-based ANN method, thereby more than doubling the data size. This imposes a high storage burden for many use cases, including RAG workloads on personal devices and semantic search over large datasets (e.g., logs or documents). In particular, running vector search locally (e.g., on laptops or workstations) is attractive because it preserves privacy\n\nand enables offline access without uploading data to the cloud *(Wang and Chau, 2024)*. However, the storage capacity of personal devices is often insufficient for large-scale embeddings and indices.\n\nTo reduce storage overhead, a common approach is to compress embeddings using lossy vector quantization methods such as product quantization (PQ) *(Jégou et al., 2011)*. Approximate distances can then be computed between the query and the compressed vectors. However, achieving small vector sizes requires a high compression ratio. For example, PQ needs about 35$\\times$ compression to reduce the vectors to 5 GB in Table1. At such a high ratio, large quantization errors degrade the downstream accuracy of vector search to levels even below keyword search with BM25. Moreover, the 15 GB HNSW index cannot be compressed using vector quantization and still burdens personal devices.\n\nAn important observation from Table 1 is that in RAG workloads, LLM generation dominates end-to-end latency (i.e., response generation takes over 20s on an RTX 4090, while vector search completes in milliseconds). This long generation time, common in complex reasoning or agentic tasks, relaxes the strict requirement for search latency. Since overall latency is bounded by generation, we can trade a small amount of search latency for substantial storage savings, enabling much more compact vector indexes. This is an attractive trade-off for personal devices or resource-constrained deployments. Motivated by this observation, we ask:\n\n> Can we design a vector index that dramatically reduces storage overhead while maintaining search accuracy and meeting reasonably relaxed latency requirements?\n\nOur solution LEANN. We present LEANN as a vector index tailored for storage-constrained environments with both system and algorithmic optimizations. LEANN can reduce the index footprint to below 5% of the original data while preserving high result accuracy and reasonable retrieval latency. At its core, LEANN is guided by two insights:\n\nThe first insight is that state-of-the-art proximity graph indexes (e.g., HNSW, which we build upon) require each query to visit only a small subset of embeddings to find its nearest neighbors. Thus, instead of storing all embedding vectors, LEANN recomputes them at query time using the same encoder as in index building. However, naive embedding recomputation can lead to significant latency overhead. To mitigate this, LEANN introduces a two-level search algorithm that uses the inaccurate approximate distances at high compression ratios to prune embedding recomputations. Moreover, LEANN also employs a dynamic batching mechanism that aggregates embedding computations across search hops over the proximity graph index to improve GPU utilization and reduce recomputation latency.\n\nWhile embedding recomputation allows removing exact vectors, the proximity graph index metadata can still be large, as shown in Table 1. For example, if a node (i.e., vector) has 64 neighbors, each adjacency list takes 256 bytes, which is already 25% in size over the typical 1 KB document chunk for original data *(Shao et al., 2024)*. Our second insight is that the high-degree nodes in a proximity graph are visited much more frequently than the low-degree nodes and thus are more important for vector search. Hence, LEANN applies a high-degree preserving graph pruning strategy, which removes the low utility edges of low-degree nodes while preserving the edges of high-degree “hub” nodes *(Munyampirwa et al., 2024)*. This substantially reduces index size without sacrificing search accuracy and efficiency.\n\nBesides the two key designs, LEANN incorporates a storage-efficient sharded merging pipeline index building strategy, which ensures that storage consumption never exceeds a small budget even when building the index for a large dataset. In addition, LEANN also supports updating the compressed index (e.g., adding new data). This significantly reduces the update time while remaining storage-efficient.\n\nWe implement LEANN on top of FAISS *(Douze et al., 2025)*, one of the most popular frameworks for ANNS, and evaluate it across four information retrieval (IR) benchmarks and beyond. Our experiment platforms include a server with NVIDIA RTX 4090 GPU *(NVIDIA, 2022)* and an M1-based Mac *(AWS, 2023)*. The results show that LEANN reduces storage consumption by more than 50$\\times$ compared to state-of-the-art vector indexes while maintaining result accuracy. When applied to RAG tasks, LEANN incurs about 10% end-to-end latency overhead (see Table 1).\n\nTo summarize, we make the following contributions:\n\n- We are the first to study the storage challenge of vector search and design LEANN, a novel storage-efficient index that performs on-the-fly embedding recomputation and applies high-degree preserving graph pruning to reduce storage while preserving accuracy.\n- We incorporate a suite of optimizations, including two-level search, dynamic batching, storage-constrained index building, and efficient index update, to make the entire pipeline both fast and storage efficient.\n- We demonstrate that LEANN achieves over 90% top-3 recall within one second while using less than 5% of the raw data storage, maintaining comparable latency for RAG workloads.\n\n2 Background\n\nVector search. To retrieve semantically related or similar objects for unstructured data (e.g., texts, images, videos), vector search is widely used. In particular, given a vector dataset $\\mathcal{X}=\\{x_{1},x_{2},\\cdots,x_{N}\\}\\subset\\mathbb{R}^{d}$ and a query vector $q\\in\\mathbb{R}^{d}$, vector search finds the top-$k$ vectors in $\\mathcal{X}$ that are the most similar to $q$, i.e.,\n\n$|\\mathcal{S}_{q}|=k\\text{ with }\\|q-x_{i}\\|\\leq\\|q-x_{j}\\|\\,\\forall x_{i}\\in\\mathcal{S}_{q},x_{j}\\in\\mathcal{X}\\setminus\\mathcal{S}_{q}.$ (1)\n\nThe similarity function can also be the inner product or cosine similarity, where larger values indicate higher similarity. However, due to the curse of dimensionality in high-dimensional spaces, exact vector search requires a linear scan *(Wang et al., 2021)*, which is costly for large datasets. As such, approximate nearest neighbor search (ANNS) is commonly used *(Malkov and Yashunin, 2018; Lempitsky, 2012)*, which trades minor result inaccuracies for substantially lower query latency. The result quality of ANNS is usually measured by recall, which is the fraction of ground-truth top-$k$ neighbors that are contained in the set $\\mathcal{S}_{q}^{\\prime}$ of returned approximate neighbors, i.e.,\n\n$Recall@K=|\\mathcal{S}_{q}\\cap\\mathcal{S}_{q}^{\\prime}|/k.$ (2)\n\nApplications such as RAG typically require a high recall (e.g., $\\geq 0.9$) for good performance *(Shen et al., 2024)*.\n\nIndexes are essential for the efficiency of vector search by confining the distance computations to a small portion of vectors. The storage cost of a vector index consists of two components, i.e., the vectors and the index metadata. Two types of vector indexes are the most popular, i.e., IVF *(Lempitsky, 2012)*) and proximity graph *(Malkov and Yashunin, 2018; Fu et al., 2019; Subramanya et al., 2019)*. IVF groups the vectors into clusters and represents each cluster with a center vector, and a query first scans the centers and then checks the vectors in a few most similar clusters. Proximity graph connects similar vectors to form a graph and conducts vector search via a best-first traversal on the graph. Although IVF is cheaper to build and requires smaller space to store the index structure, proximity graph achieves the SOTA efficiency for vector search in that it requires much fewer distance computations *(Subramanya et al., 2019)*.\n\nBest-first search on proximity graph.\n\nThe variants of proximity graph index (e.g., HNSW *(Malkov and Yashunin, 2018)*, NSG *(Fu et al., 2019)*, Vamana *(Subramanya et al., 2019)*) differ in their edge connection rules, but the query processing algorithm is similar. Algorithm 1 illustrates the best-first search on the proximity graph. The search maintains a bounded priority queue $C$ of candidate nodes, ordered by their distances to the query $q$. At each exploration step (Line 5), the algorithm reads (but does not remove) the closest unvisited node $u$ from $C$ and explores its neighbors. For each neighbor whose distance has not been computed, the algorithm extracts the embedding, computes its distance to $q$, and inserts the neighbor into $C$ if the queue is not full or if the neighbor is closer than the tail entry of $C$. The parameter $ef$ bounds the queue size and acts as a quality knob: a larger $ef$ improves recall at the cost of more distance computations. The search terminates once all the nodes in $C$ have been visited. Empirically, graph-based indexes achieve high recall with only $\\mathcal{O}(\\log N)$ embedding extractions and distance computations. This is because the graph traversal can quickly converge on the neighbors of the query by moving to more similar neighbors in each step.\n"
  },
  {
    "title": "3 LEANN Overview",
    "node_id": "0002",
    "summary": "The text describes the LEANN system, which handles vector search through an offline index construction and an online query serving process. The offline stage involves computing embeddings, building a graph-based vector index with pruning for storage efficiency, and creating a product quantization (PQ) table for fast distance estimation. It can also partition the graph to meet storage budgets, ultimately persisting pruned graph adjacency lists and a PQ-compressed embedding table. The online stage uses a two-level search strategy: first, approximate distances are calculated using PQ embeddings, and then exact embeddings are recomputed on demand for promising candidates, with dynamic batching enhancing GPU utilization. The system ranks results by exact distance and offers a lightweight update pipeline and an optional embedding cache. LEANN significantly reduces storage by up to 50x compared to dense indexes. Its use cases include vector search on resource-constrained devices, managing data lakes with cold datasets, and handling datasets with skewed access patterns by storing popular entries' exact vectors and recomputing others. Algorithm 2 details the two-level search process.",
    "start_index": 3,
    "end_index": 4,
    "text": "# 3 LEANN Overview\n\nFigure 1 shows the end-to-end workflow of LEANN, which includes offline index construction and online query serving.\n\nOffline stage Given a dataset of items, such as chunked unstructured text, LEANN computes embeddings and builds a graph-based vector index. To minimize storage, it applies a graph pruning algorithm that preserves high-degree nodes (§5) and discards dense embeddings, retaining only the pruned graph structure. During construction, LEANN also builds a lightweight product quantization (PQ) table that stores approximate embeddings for fast distance estimation during query processing (§4). Optionally, if a peak storage budget is specified, LEANN adopts a graph partitioning-based build strategy (§6) to keep the storage footprint within this bound by constructing and merging shards sequentially. At the end of the offline stage, LEANN persists two compact components: (i) the pruned graph adjacency lists and (ii) the PQ-compressed embedding table.\n\nOnline stage. When a query arrives, LEANN searches over the pruned graph using Algorithm 1. To accelerate query processing, LEANN employs a two-level search strategy that first computes lightweight approximate distances using PQ embeddings and then recomputes exact embeddings on demand for the most promising candidates via the local embedding generator. During recomputation, LEANN ap\n\n![img-0.jpeg](img-0.jpeg)\nFigure 1. LEANN System Diagram. The system combines high-degree preserving graph pruning for minimal storage footprint with graph-based recomputation and two-level search with dynamic batching for efficient query processing (Steps 1-4).\n\nplies dynamic batching to group multiple candidate nodes across exploration steps, improving GPU utilization and reducing end-to-end latency. Finally, the system ranks all visited nodes by their exact distance to the query and returns the top results to the downstream task. LEANN also provides a lightweight update pipeline for dynamic index maintenance (§6) and, if disk capacity allows, an optional embedding cache to store frequently accessed nodes and avoid redundant recomputation.\n\nStorage composition. Across both stages, LEANN stores compact structures. For  $N$  data chunks (nodes), the pruned graph requires  $O(N \\times |D|)$  integer entries, where  $|D|$  denotes the average node degree. The PQ table employs a  $100 \\times$  smaller codebook than the original FP32 embeddings, occupying  $O(4N \\times dim / 100)$  bytes (e.g.,  $dim = 768$ ). Together, these components reduce storage by up to  $50 \\times$  compared to conventional dense indexes.\n\nUse cases. LEANN can conduct vector search on user devices (e.g., laptops and personal servers), on which storage is highly limited. Our experiment evaluation also focuses on this use case. LEANN may also be used for data lakes, which contain many datasets, and some cold datasets are queried infrequently (Mageirakos et al., 2025). Storing indexes for these cold datasets incurs high space overheads, while recomputing embeddings for them is inexpensive due to low query frequency. Similarly, LEANN can handle datasets whose embeddings have skewed access patterns, e.g., for recommendation and content search, popular entries are more likely to become the results of vector search (Mohoney et al., 2023). For these datasets, LEANN may store exact vectors for the popular entries and use embedding recomputation for the cold entries to reduce storage.\n\nAlgorithm 2 Two-Level Search\n\n1: Input: query  $q$ , entry point  $p$ , re-ranking ratio  $\\alpha$ , result size  $k$ , search queue length  $ef$\n2: Output:  $k$  closest neighbors to  $q$\n3: Init size-ef priority queue  $EQ$  with  $(p, \\mathrm{Dist}(q, x_p))$\n4: Init empty approximate priority queue  $AQ$\n5: while  $EQ$  has unvisited node do\n6: Read the closest unvisited node  $u$  from  $EQ$\n7: Mark  $u$  as visited\n8: for each neighbor  $v$  of  $u$  do\n9: if approximate distance to  $q$  not computed then\n10: Extract approximate embedding  $\\hat{x}_v$  for  $v$\n11: Insert  $(v, \\mathrm{Dist}(q, \\hat{x}_v))$  into  $AQ$\n12:  $C\\gets$  top  $\\alpha \\%$  candidates in  $AQ$  , excluding EQ\n13: for each  $c\\in C$  do\n14: Recompute embedding  $x_{c}$\n15: Try to insert  $(c, \\mathrm{Dist}(q, x_c))$  into  $EQ$\n16: return The  $k$  nodes with smallest distances in  $EQ$\n"
  },
  {
    "title": "4 GRAPH-BASED RECOMPUTATION",
    "node_id": "0003",
    "summary": "This section details an efficient graph-based recomputation pipeline designed to reduce computation and maximize GPU utilization. It introduces a two-level search with hybrid distance computation, interleaving approximate and exact distance calculations to maintain search quality while pruning unnecessary computations. This approach addresses the inaccuracies of solely relying on approximate distances for initial graph traversal. The pipeline also incorporates dynamic batching for recomputation, relaxing strict data dependencies to accumulate nodes across multiple exploration steps, thereby increasing batch sizes and significantly improving GPU throughput at the cost of minor staleness.",
    "start_index": 4,
    "end_index": 5,
    "text": "# 4 GRAPH-BASED RECOMPUTATION\n\nIn this section, we introduce our efficient recomputation pipeline, which reduces the number of nodes involved in recomputation (§4.1) and fully utilizes GPU resources during the process (§4.2).\n\n## 4.1 Two-Level Search with Hybrid Distance\n\nMotivation. LEANN stores PQ codes for all vectors to enable approximate distance computation. Existing systems such as DiskANN search the proximity graph using these approximate distances and then re-rank the top candidates with exact distances, e.g., re-ranking the top-100 approximate neighbors for top-10 results. However, this approach is problematic for LEANN because our PQ codes use a high compression ratio for compact storage, leading to large quantization errors. In particular, the approximate\n\ndistances can lead the graph traversal to detours by visiting sub-optimal candidates, which prolongs search time. Moreover, some ground-truth neighbors may be missed due to sub-optimal candidates, and re-ranking more approximate neighbors will not improve recall in this case (see Figure 4). To tackle this problem, we interleave approximate and exact distance computations rather than isolating them as in existing systems. Specifically, we use exact distances to select candidates to visit so that the graph traversal maintains high quality, while approximate distances are used to prune unnecessary exact computations, achieving accuracy and efficiency at the same time.\n\nSolution. Algorithm 2 outlines the complete procedure. At each exploration step, LEANN first computes approximate distances for all neighbors using PQ (Line 11) and maintains an approximate queue  $(AQ)$  that stores these values for all explored nodes. Instead of recomputing every neighbor's embedding, we define a re-ranking ratio  $\\alpha$  and extract the top  $\\alpha\\%$  of nodes from  $AQ$ , excluding those already in the exact queue  $(EQ)$ . The selected subset  $C$  (Line 12) is then recomputed exactly, and each node is inserted into  $EQ$  for further exploration. This hybrid strategy significantly reduces recomputation without sacrificing accuracy.\n\nDiscussion. In practice, LEANN uses a PQ table with a  $100 \\times$  smaller codebook, representing embeddings in  $O(4N \\times dim / 100)$  bytes (with  $dim = 768$  in our setup). These approximate distances provide an inexpensive yet effective signal for early filtering, and recomputation is reserved only for a small fraction of top-ranked candidates. Although PQ introduces quantization errors, selective exact recomputation restores ranking fidelity and ensures retrieval quality. The method generalizes easily to other forms of approximation, such as using distilled embedding models or link-and-code representations (Douze et al., 2018).\n\n## 4.2 Dynamic Batching for Recomputation\n\nMotivation. In the naive approach, embeddings are recomputed one by one for each neighbor node, as shown in Line 14 of Algorithm 2. To better utilize the GPU, LEANN batches all neighbor nodes within an exploration step so their embeddings are recomputed together. However, even with this optimization, each batch remains small—limited by the degree of the current node  $u$ . The problem becomes more pronounced in the two-level search algorithm (Line 12), where the candidate set per step is even smaller.\n\nSolution. To further improve GPU utilization, LEANN introduces a dynamic batching strategy that relaxes the strict\n\n![img-1.jpeg](img-1.jpeg)\n(a) Node (out)-degree distribution\n\n![img-2.jpeg](img-2.jpeg)\n(b) Node access probability.\nFigure 2. HNSW graph analysis reveals skewed access and degree distributions, with node degrees capped at 60 by HNSW.\n\ndata dependency in best-first search (Algorithm 1). While this introduces slight staleness in the exploration order, it enables batching across multiple exploration steps, increasing the effective batch size and improving throughput.\n\nSpecifically, LEANN dynamically collects a group of the closest candidates from the priority queue. The algorithm accumulates nodes requiring recomputation until a target batch size is reached (e.g., 64), which can be efficiently determined through lightweight offline profiling. This dynamic batching mechanism integrates naturally with the two-level search strategy described in §4.1: in practice, LEANN accumulates nodes in the set  $C$  across iterations until the predefined batch threshold is reached, then recomputes embeddings for all nodes in  $C$  together.\n\nThis dynamic batching approach allows LEANN to batch nodes across multiple graph exploration steps regardless of individual node degrees, trading off slight staleness for significantly improved GPU utilization compared to single-step processing.\n"
  },
  {
    "title": "5 COMPACT GRAPH STRUCTURE",
    "node_id": "0004",
    "summary": "This text describes LEANN's approach to reducing graph index metadata storage overhead through a high-degree preserving graph pruning algorithm. The core problem is to minimize recomputation cost while adhering to a disk usage budget and maintaining retrieval accuracy. Unlike naive pruning methods that harm connectivity, LEANN identifies high-degree nodes as crucial 'navigation hubs' and preserves their connections. The algorithm assigns different degree limits (m for most nodes, M for a small percentage of high-degree nodes) and prunes edges from low-degree nodes. This strategy aims to maintain graph navigability and search quality by preserving essential hub nodes while significantly reducing overall storage requirements.",
    "start_index": 5,
    "end_index": 6,
    "text": "# 5 COMPACT GRAPH STRUCTURE\n\nWith the two-level search and dynamic batching mechanisms optimizing recomputation latency, we now examine how LEANN further reduces storage overhead in graph index metadata through a high-degree preserving graph pruning algorithm. As noted in §3, although LEANN eliminates the need to store exact embeddings by recomputing them at query time, the graph metadata that guides the search still incurs significant storage cost (see Table 1). In fact, even with embeddings, the index metadata alone can exceed  $30\\%$\n\nof the total storage *(Severo et al., 2025)*.\n\nProblem Formulation. Given a disk usage constraint $B$, LEANN aims to prune the graph index so that the metadata storage remains within budget while maintaining retrieval accuracy. Formally, the optimization problem is:\n\n$\\min\\quad T(G_{1})$ $=\\sum_{i=1}^{ef}|V_{i}|$ (3)\ns.t. $\\text{Space}(G_{1})$ $=\\sum_{v\\in V(G_{1})}\\deg(v)\\,s_{\\text{edge}}\\leq B,$\n$\\text{Acc}(G_{1})$ $\\geq\\tau$\n\nHere, $G_{1}$ is the pruned graph, and $|V_{i}|$ is the number of nodes recomputed in each exploration step during search using $G_{1}$. A smaller $T(G_{1})$ indicates fewer recomputations and thus lower query latency. $\\text{Space}(G_{1})$ denotes the metadata size of the graph, stored in a compressed sparse row (CSR) format, which records each node’s outgoing neighbor IDs. $\\deg(v)$ is the out-degree of node $v$, and each stored neighbor ID takes 4 bytes. The goal is to minimize recomputation cost while keeping the graph within the storage budget $B$ and accuracy (recall) above the threshold $\\tau$.\n\nMotivation. There are two naive ways to shrink the proximity graph index: (1) randomly removing edges, and (2) lowering the degree limit for each node. Both approaches significantly degrade search accuracy even under mild size reductions, as shown in Figure 6, because they harm graph connectivity, which is crucial for effective traversal. From Figure 2, we observe that the edges are not equally important: a small fraction of nodes have high degrees (i.e., approaching or at the degree limit), and these nodes are accessed much more frequently than the low-degree nodes. These high-degree nodes essentially serve as the “navigation hubs” for graph traversal, and similar phenomena are also observed in *(Munyampirwa et al., 2024)*. As such, we preserve the edges for the high-degree nodes to ensure good navigability of the proximity graph and conduct edge pruning for the low-degree nodes.\n\nSolution: Our key insight is that preserving a small set of hub nodes is sufficient to maintain search performance. Following prior work *(Ren et al., 2020; Munyampirwa et al., 2024)*, high-degree nodes serve as the backbone of the graph’s connectivity; thus, LEANN focuses on retaining these hubs while reducing the overall number of edges. Algorithm 3 outlines this high-degree preserving graph pruning strategy.\n\nWe assign degree thresholds based on node importance: most nodes are limited to a lower degree $m$, while a small fraction of nodes ($\\beta\\%$) can retain up to $M$ connections (Line 7). Empirically, we set $m=M/5$ and determine $M$ for a given storage budget $B$ through offline profiling. We use node degree as a proxy for node importance and select the top $\\beta\\%$ of nodes by degree (Line 4). Preserving only the top 2% of high-degree nodes significantly reduces edge count while maintaining high retrieval accuracy.\n\nAlgorithm 3 High-Degree Preserving Graph Pruning\n1: Input: Original graph $G$ with vertex set $V$; construction queue length $efC$; maximum degree $M$ for high-degree nodes; lower degree $m$ for others ($m<M$); proportion of high-degree nodes $\\beta$\n2: Output: Pruned graph $G_{1}$\n3: Init $D[v]\\leftarrow\\deg(v)$ for all $v\\in V$; $G_{1}\\leftarrow\\emptyset$\n4: $V^{*}\\leftarrow$ nodes with the top $\\beta\\%$ highest degrees in $D$\n5: for $v\\in V(G)$ do $\\triangleright$ Construct $G_{1}$\n6: $W\\leftarrow$ Search($v$, $efC$) $\\triangleright$ See Algorithm 1\n7: if $v\\in V^{*}$ then $M_{0}\\leftarrow M$\n8: else $M_{0}\\leftarrow m$\n9: Select up to $M_{0}$ neighbors from $W$\n10: Add bidirectional edges between $v$ and neighbors\n11: If $\\deg(u)>M$ for any neighbor $u$, shrink to $M$\n\nAlgorithm 4 The number of outgoing connections when a node is first inserted into the graph (Line 8), we allow all nodes to form bidirectional links with newly inserted nodes up to the higher threshold $M$ (Line 11), instead of the lower limit $m$. This design ensures that each node retains the opportunity to connect with high-degree hub nodes, thereby preserving graph navigability with minimal impact on search quality.\n"
  },
  {
    "title": "6 Index Building and Update",
    "node_id": "0005",
    "summary": "The text describes LEANN's approach to building and updating vector indexes efficiently. For index building, it employs a storage-efficient sharded merging pipeline that uses k-means for soft assignment, shard-wise graph construction, and a merging strategy for the final graph, all while minimizing peak storage. For updates, LEANN optimizes single-node insertions and deletions (using soft deletion) to reduce computational overhead, and supports batched add operations by buffering and asynchronously merging new data. The document also highlights LEANN's significantly lower storage footprint compared to other methods and its ability to operate within typical RAM constraints, contrasting it with memory-heavy alternatives like HNSW.",
    "start_index": 6,
    "end_index": 7,
    "text": "# 6 Index Building and Update\n\nStorage-Efficient Index Build. The naive index construction in LEANN requires precomputing embeddings for all objects to build the graph structure. Although these embeddings are discarded afterward to reduce storage at query time, the peak storage usage during construction can still be substantial. To address this, LEANN introduces a simple yet effective sharded merging pipeline strategy that builds the index efficiently under a user-specified storage constraint while preserving graph quality. The sharded merging pipeline process consists of three stages: *Soft assignment with k-means.* We first run k-means on a small subset of the corpus to obtain $k$ centroids. Each object is then embedded and assigned to its two nearest centroids. This is performed sequentially; after assignment, embeddings are immediately discarded, and only the two-centroid mapping for each passage is retained. *Shard-wise graph construction.* After the assignment, we build the graph index separately for each of the $k$ shards. For each shard, embeddings are recomputed, the graph is constructed, and the embeddings are discarded. Since each passage belongs to two shards, the merged graph achieves good global connectivity. *Graph merging.* We then merge the $k$ shard graphs into a single\n\n![img-3.jpeg](img-3.jpeg)\nFigure 3. Storage consumption of different vector index methods on the RPJ-Wiki dataset. The black dashed line marks the raw dataset size (76 GB), while the red dashed line shows the typical RAM capacity (32 GB, RTX 4090, following our testbed configuration in §7.1). Memory-heavy methods like HNSW exceed this RAM limit and cannot run on such hardware. LEANN achieves the lowest storage footprint at only  $5\\%$  of the original dataset size.\n\nstructure. For nodes appearing in two shards, we assign the higher of their two HNSW levels as the final layer. For lower layers, we merge their edge lists and randomly drop edges when the node degree exceeds  $M$ . This heuristic yields a well-connected, high-quality graph (see Figure 10 in Appendix D); more advanced merging strategies, such as RNG-based pruning (Jaromczyk &amp; Toussaint, 1992), are left for future work.\n\nEfficient Index Update. We enable efficient updates in LEANN through a series of optimizations that substantially reduce computational and storage overhead. For a single update request, the naive recomputation procedure has a complexity of  $\\mathcal{O}(M \\cdot efC + efC^2 + M^3)$ , arising from repeated embedding calculations and neighbor maintenance. These three terms correspond to neighbor search, neighbor selection, and reverse-edge selection and updates. LEANN improves efficiency through lightweight embedding, caching and a simplified selection strategy, eliminating redundant computations and reducing the total cost to  $\\mathcal{O}(M \\cdot efC)$  while preserving graph connectivity and quality. For deletions, LEANN employs soft deletion by marking nodes as inactive rather than removing them from the graph structure, preserving connectivity while avoiding costly graph reorganization. Details are provided in Appendix B.\n\nBeyond single-node insertion, LEANN supports batched add operations with system-level optimizations. Incoming embeddings are temporarily buffered, and upon receiving a query, LEANN scans the buffer and merges its results with those from the existing graph. The buffered entries are then inserted asynchronously, amortizing update costs. This design minimizes computation and peak storage usage while maintaining low search latency.\n"
  },
  {
    "title": "7 EVALUATION",
    "node_id": "0006",
    "summary": "# 7 EVALUATION\n\nWe begin by describing the experimental setup in §7.1. Then, in §7.2, we present the main results and answer the following key questions: (1) What is the storage overhead of different indexing methods? (2) What is the latency of various vector search methods and the end-to-end RAG pipeline using them? (3) What is the end-to-end RAG accuracy achieved by different methods? Finally, in §7.3, we conduct comprehensive ablation studies to evaluate the effectiveness of each component in LEANN.\n",
    "start_index": 7,
    "end_index": 7,
    "text": "# 7 EVALUATION\n\nWe begin by describing the experimental setup in §7.1. Then, in §7.2, we present the main results and answer the following key questions: (1) What is the storage overhead of different indexing methods? (2) What is the latency of various vector search methods and the end-to-end RAG pipeline using them? (3) What is the end-to-end RAG accuracy achieved by different methods? Finally, in §7.3, we conduct comprehensive ablation studies to evaluate the effectiveness of each component in LEANN.\n",
    "nodes": [
      {
        "title": "7.1 Experiment Settings",
        "node_id": "0007",
        "summary": "The text details the experiment settings, including the workloads used (RPJ-Wiki for datastore and several QA datasets for retrieval tasks), the construction of the datastore with specific parameters, and the hardware testbeds (a workstation and an AWS EC2 M1 Mac instance). It also lists the baselines for comparison, which encompass graph-based, cluster-based, quantization-based, and lexical retrieval methods, with configurations provided in an appendix.",
        "start_index": 7,
        "end_index": 7,
        "text": "## 7.1 Experiment Settings\n\nWorkloads: Datastore and QA dataset. We construct the retrieval datastore using the RPJ-Wiki dataset (Computer, 2023), a widely used corpus comprising approximately 76 GB of raw Wikipedia text. Following prior work (Shao et al., 2024), we segment the text into 256-token chunks and generate an embedding for each chunk using CONTRIEVER (Izacard et al., 2021), yielding 768-dimensional vectors. In total, we obtain 60 million ( $N = 60\\mathrm{M}$ ) passages, producing about 173 GB of embeddings. For the QA datasets, we adopt four standard benchmarks commonly used in RAG and open-domain retrieval: NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), GPQA (Rein et al., 2024), and HotpotQA (Yang et al., 2018). Beyond the Wikipedia QA task, we further evaluate on FinanceBench (Islam et al., 2023) for financial document retrieval, the Enron Email Corpus (Ryan et al., 2024) for email retrieval, and LAION (Schuhmann et al., 2021) for image data retrieval.\n\nTestbed. We evaluate our system on two hardware platforms. The first is a workstation with an NVIDIA RTX 4090 GPU (NVIDIA, 2022), 32GB RAM, and a 1 TB disk running WSL2. The second is an AWS EC2 M1 Mac instance (AWS, 2023) with an Apple M1 Ultra (Arm64) processor, macOS, 128GB RAM and a 512 GB EBS volume.\n\nBaselines. We compare LEANN against the following baselines: HNSW (Malkov &amp; Yashunin, 2018), IVF, DiskANN (Subramanya et al., 2019), IVF-Disk, IVF-Recompute (Seemakhupt et al., 2024), PQ Compression (Jégou et al., 2011), and BM25 (Craswell et al., 2021). These baselines cover graph-based, cluster-based, quantization-based, and lexical retrieval paradigms. Detailed configurations are provided in Appendix C.1.\n"
      },
      {
        "title": "7.2 Main results",
        "node_id": "0008",
        "summary": "The text presents a comprehensive evaluation of LEANN against various baseline methods for storage consumption, vector search latency, and end-to-end RAG accuracy. LEANN demonstrates significant storage savings, maintaining overhead below 5% of raw dataset size, making it practical for personal devices. While some methods achieve storage efficiency, LEANN uniquely offers both high speed and accuracy. Its retrieval latency is acceptable, with generation dominating total response time. LEANN outperforms BM25 and PQ in downstream RAG tasks, achieving comparable accuracy to HNSW at a 90% recall level while using substantially less storage. The text also details storage overheads of different methods, explains latency differences based on recomputation complexity, and includes results on different platforms and optimization techniques. Finally, it compares pruned graph quality and degree distributions, highlighting LEANN's ability to preserve important graph properties.",
        "start_index": 7,
        "end_index": 9,
        "text": "## 7.2 Main results\n\nStorage consumption. We compare the storage consumption of all baselines and LEANN in Figure 3. Among all methods, only LEANN and IVF-Recompute maintain total storage overhead below  $5\\%$  of the raw dataset size (76 GB).\n\nTable 2. Vector search and end-to-end RAG latency across datasets on RTX 4090. Latency is reported at  $90\\%$  recall. Results for PQ and BM25 are omitted as they fail to reach this accuracy (their downstream accuracy is also low in Figure 4). Results for HNSW and IVF are measured on a server with larger memory, as they cause OOM on the local RTX 4090. Overhead (\\%) denotes the ratio of retrieval latency to total pipeline latency (retrieval / [retrieval + generation]).\n\n|  Dataset | Generation (s) | Method | Retrieval (s) | Overhead (%) | Dataset | Generation (s) | Method | Retrieval (s) | Overhead (%)  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  NQ | 20.86 | HNSW | 0.05 | 0.20 | GPQA | 69.60 | HNSW | 0.04 | 0.06  |\n|   |   |  IVF | 2.55 | 10.90 |   |   | IVF | 0.17 | 0.25  |\n|   |   |  DiskANN | 0.03 | 0.10 |   |   | DiskANN | 0.03 | 0.05  |\n|   |   |  IVF-Disk | 3.44 | 14.10 |   |   | IVF-Disk | 0.06 | 0.09  |\n|   |   |  IVF-Recompute | 307.61 | 93.60 |   |   | IVF-Recompute | 21.88 | 23.20  |\n|   |   |  PQ Compression | - | - |   |   | PQ Compression | - | -  |\n|   |   |  BM25 | - | - |   |   | BM25 | - | -  |\n|   |   |  LEANN | 2.48 | 10.60 |   |   | LEANN | 1.12 | 1.60  |\n|  TriviaQA | 17.17 | HNSW | 0.04 | 0.20 | HotpotQA | 23.28 | HNSW | 0.05 | 0.20  |\n|   |   |  IVF | 3.54 | 17.10 |   |   | IVF | 3.87 | 14.20  |\n|   |   |  DiskANN | 0.06 | 0.30 |   |   | DiskANN | 0.11 | 0.50  |\n|   |   |  IVF-Disk | 3.65 | 17.50 |   |   | IVF-Disk | 5.05 | 17.80  |\n|   |   |  IVF-Recompute | 399.12 | 95.90 |   |   | IVF-Recompute | 429.46 | 94.80  |\n|   |   |  PQ Compression | - | - |   |   | PQ Compression | - | -  |\n|   |   |  BM25 | - | - |   |   | BM25 | - | -  |\n|   |   |  LEANN | 2.96 | 14.70 |   |   | LEANN | 7.12 | 23.40  |\n\nTable 3. Storage usage and retrieval latency overhead of LEANN on personal datasets (RTX 4090). Overhead (\\%) follows the definition in Table 2, and Storage Savings (\\%) denote the storage consumption of LEANN relative to HNSW.\n\n|  Dataset | Generation (s) | Retrieval (s) | Overhead (%) | Storage Savings (%)  |\n| --- | --- | --- | --- | --- |\n|  FinanceBench | 46.0 | 1.5 | 3 | 97  |\n|  Enron | 22.3 | 1.9 | 8 | 98  |\n|  LAION | 6.6 | 1.6 | 20 | 97  |\n\nMost existing systems incur substantial overhead, up to  $2.5 \\times$  the raw data size, making them impractical for deployment on personal devices. HNSW stores every dense embedding along with its graph connections, where each node contains a 768-dimensional embedding vector and padding for up to 60 neighbors (the maximum degree). DiskANN further amplifies this overhead due to its sector-aligned layout: each node's embedding ( $768 \\times 4$  bytes) and neighbor list ( $60 \\times 4$  bytes) are padded to 4KB SSD sectors. It also requires an additional 30GB for PQ embeddings, yielding the largest footprint (270 GB) among all methods. IVF and IVF-Disk exhibit similar overheads, both dominated by the cost of storing full embeddings. For BM25, the index size scales with the vocabulary and is roughly comparable to the raw corpus size in our setup. PQ compresses embeddings to a similar size as LEANN (5GB) but requires an additional 15GB for graph index metadata. In contrast, LEANN stores only a compact graph and highly compressed PQ embeddings, resulting in less than  $5\\%$  additional storage overhead. Among all baselines, IVF-Recompute achieves the smallest footprint by storing only IVF centroids on disk.\n\nWe also compare LEANN against the most widely used HNSW index in Table 3, showing that it achieves over  $97\\%$  storage savings across diverse datasets.\n\nLatency evaluation for vector search and RAG. We evaluate the latency of vector search and end-to-end RAG across different methods in Table 2 and Table 3. We measure the per-request retrieval latency required to achieve  $90\\%$  recall (Recall@3, defined in §2) and the subsequent generation time. Detailed measurement procedures are provided in Appendices C.2-C.3.\n\nFirst, we show that second-level retrieval latency is acceptable. The generation phase dominates the total response time, typically exceeding 10s and reaching up to 70s, so the design choice in LEANN to trade a small amount of latency for substantial storage savings is well justified.\n\nSecond, while several methods achieve storage efficiency, only LEANN delivers both high speed and accuracy. Among all baselines, only BM25, PQ, IVF-Recompute, and LEANN maintain storage overhead below the size of the raw dataset, as shown in Figure 3. However, BM25 and PQ exhibit low retrieval accuracy and fail to reach  $90\\%$  recall. IVF-Recompute attains high recall but requires up to two orders of magnitude longer retrieval time than LEANN (up to  $200\\times$  slower).\n\nThis difference arises because LEANN employs a graph-based index with  $\\mathcal{O}(\\log N)$  embedding recomputation, while IVF-Recompute performs  $\\mathcal{O}(\\sqrt{N})$  recomputations (Wang et al., 2021) ( $N = 60\\mathrm{M}$  in our experiments). Additional latency optimizations described in §4 further\n\n![img-4.jpeg](img-4.jpeg)\nFigure 4. Comparison of Exact Match and F1 scores for downstream RAG tasks across four methods: keyword search (BM25), PQ-compressed vector search, HNSW, and LEANN. HNSW and LEANN are configured to achieve a target recall of  $90\\%$ , while the PQ baseline is given extended search time to reach its highest possible recall. Here we use Qwen3-4B as the generation model.\n\ncontribute to LEANN's performance advantage.\n\nFinally, although LEANN's standalone vector search latency is higher than graph-based baselines such as HNSW and DiskANN, we show that LEANN introduces negligible latency overhead when integrated into the full RAG pipeline on personal devices, while using far less storage. As shown in Table 2 and Table 3, LEANN consistently adds less than  $20\\%$  latency overhead to the end-to-end retrieval and generation process. For reasoning-intensive tasks such as the graduate-level QA benchmark GPQA, the additional overhead introduced by LEANN remains under  $3\\%$ , as the model's long chain-of-thought generation dominates total latency.\n\nWe include Mac latency results in Table 4 (Appendix C.4), using the same setup as Table 2. Despite the Mac's lower TFLOPS, all previous conclusions hold, demonstrating LEANN's generalization across platforms.\n\nAccuracy of downstream RAG applications. To evaluate RAG accuracy, we compare all retrieval methods on four QA datasets using Exact Match (EM) and F1 as metrics, as shown in Figure 4. LEANN achieves the highest downstream QA performance among all methods.\n\nAs shown in Figure 4, LEANN consistently outperforms BM25 and PQ across all datasets. It improves EM by up to  $11.8\\%$  over BM25 and  $11.3\\%$  over PQ, and F1 by up to  $12.0\\%$  and  $11.1\\%$ , respectively. The gains are most pronounced on factual answering benchmarks such as NQ and TriviaQA, where accurate semantic retrieval provides clear benefits. In contrast, the improvement is smaller on GPQA and HotpotQA. This is because RPJ-Wiki datastore is partially out-of-distribution for GPQA, which contains graduate-level questions that are less supported by Wikipedia content, and HotpotQA requires multi-hop reasoning, while our setup performs only single-hop retrieval.\n\nFinally, when a target recall level (i.e.,  $90\\%$ ) is enforced, the downstream accuracy of LEANN matches that of HNSW, confirming that our method preserves accuracy while achieving substantial storage savings.\n\n![img-5.jpeg](img-5.jpeg)\nFigure 5. Speedup achieved by different optimization techniques described in §4 when evaluated on four datasets to reach the same recall level. Two-level refers to the optimization in §4.1, while Batch corresponds to §4.2.\n\n![img-6.jpeg](img-6.jpeg)\nFigure 6. Comparison of pruned graph quality against two heuristic methods using the datastore in §7.1. At each recall target on the NQ dataset, we vary the search queue length  $ef$  to determine the minimum number of nodes that must be recomputed (lower is better). The dashed gray line represents the original HNSW graph as a baseline reference, which uses twice the storage (i.e., average degree) of the pruned methods.\n\n![img-7.jpeg](img-7.jpeg)\nFigure 7. Comparison of (out-)degree distributions among the original graph, our pruning method, and two heuristic baselines. Similar to Figure 6, the gray curve represents the original HNSW graph, which has twice the size of the others. Only our pruning method successfully preserves the high-degree nodes.\n"
      },
      {
        "title": "7.3 Ablation Studies and Micro Benchmarks",
        "node_id": "0009",
        "summary": "This section presents ablation studies and micro benchmarks for LEANN, evaluating its latency optimization techniques like two-level search and dynamic batching, which yield significant speedups. It compares LEANN's graph pruning method with baselines, demonstrating its effectiveness in preserving high-degree nodes and reducing recomputation needs. The section also details latency improvements for index updates and refers to additional experiments in the appendix covering index building, embedding model size, caching strategies, and query latency decomposition.",
        "start_index": 9,
        "end_index": 10,
        "text": "## 7.3 Ablation Studies and Micro Benchmarks\n\nLatency optimization techniques. To evaluate the latency optimizations in LEANN described in §4, we incrementally enable each component while maintaining a fixed target recall across multiple datasets. Starting from a naive HNSW recomputation baseline, adding the two-level search mechanism (§4.1) yields an average  $1.4 \\times$  speedup (up to  $1.6 \\times$ ) by\n\nreducing the number of nodes requiring recomputation, with LEANN enabling lightweight distance estimation without invoking the embedding generator. Incorporating dynamic batching further improves GPU utilization during recomputation, increasing the average speedup to  $1.8 \\times$  and the peak to  $2.0 \\times$ . Among all datasets, HotpotQA gains the most from dynamic batching, as its longer search paths allow more effective grouping of multi-hop requests.\n\nAlternative graph pruning methods. We compare our high-degree preserving graph pruning algorithm with two baselines: (1) Random Prune, which randomly removes  $50\\%$  of edges from the original graph; and (2) Small  $M$ , which constrains the maximum out-degree during graph construction, yielding an average degree half that of the original graph. We evaluate graph quality by measuring the number of nodes that must be recomputed to achieve a given recall target, as shown in Figure 6. In LEANN, latency is dominated by embedding recomputations, making this metric a proxy for retrieval latency. The original graph has an average degree of 18. All three pruning methods, ours and the two baselines, are applied to reduce the average degree by half, from degree of 18 to 9, thereby halving the graph's storage overhead. As shown in Figure 6, our pruning method introduced in §5 achieves performance comparable to the original unpruned graph, while using only half the edges. To reach the same recall levels, Random Prune requires up to  $1.8 \\times$  more nodes to recompute, while Small M requires up to  $5.8 \\times$  more nodes to recompute. We omit the Small M results at the  $94\\%$  and  $96\\%$  recall targets, as it fails to reach these accuracy levels.\n\nDegree distribution in pruned graphs. To better understand the effectiveness of our pruning strategy, we analyze the out-degree distributions of the original graph, our approach, Random Prune, and Small M. As discussed in §5, our design explicitly aims to preserve high-degree \"hub\" nodes. As shown in Figure 7, it successfully retains a substantial number of such nodes, whereas the other two baselines fail to do so. This underscores the critical role of hub nodes in supporting efficient graph-based vector search, a finding that aligns with insights from prior work (Ren et al., 2020; Munyampirwa et al., 2024; Manohar et al., 2024).\n\nIndex update. Figure 8 shows how latency changes as we incrementally enable the optimizations of the LEANN add operation described in Appendix B. We achieve up to a  $63.3 \\times$  speedup over the naive method, consistent with our theoretical analysis. On the right, introducing a buffer to delay batched additions further improves search speed while maintaining accuracy.\n\nMore experiments. We provide additional experiment results in Appendix D. In Appendix D.1, we show that our sharded merging pipeline preserves the quality of proximity\n\n![img-8.jpeg](img-8.jpeg)\nFigure 8. Comparison of update methods\n\ngraph index while significantly reducing the peak storage during index-building. In Appendix D.2, we show that using a smaller embedding model further accelerates LEANN without compromising accuracy. We also show that caching the exact embeddings for some hot objects effectively reduces query latency in Appendix D.3 and decompose the query latency of LEANN in Appendix D.4.\n"
      }
    ]
  },
  {
    "title": "8 RELATED WORK",
    "node_id": "0010",
    "summary": "This section reviews related work in resource-constrained vector search, detailing various approaches to reduce memory costs, including disk-based systems, compressed embeddings on disk, online embedding generation, and embedding compression techniques. It also discusses vector search applications on personal devices, such as on-device RAG, personalized recommenders, and vision-based search, highlighting the need for efficient and low-overhead solutions like LEANN.",
    "start_index": 10,
    "end_index": 11,
    "text": "# 8 RELATED WORK\n\nResource-constrained vector search. Many works aim to reduce the memory cost of vector search. Disk-based systems like DiskANN (Subramanya et al., 2019) store vectors and graphs on disk with compressed in-memory embeddings for navigation. Starling (Wang et al., 2024) improves disk I/O, and FusionANNS (Tian et al., 2025) coordinates SSD, CPU, and GPU to lower cost. AiSAQ (Tatsuno et al., 2024) and LM-DiskANN (Pan et al., 2023) further cut DRAM use by keeping compressed embeddings on disk. EdgeRAG (Seemakhupt et al., 2024) generates embeddings online via an IVF-based index but still suffers high storage and recomputation overhead. MicroNN (Zhang et al., 2025) and ObjectBox (ObjectBox, 2025) are optimized for personal devices but still require storing all embeddings. Embedding compression methods like PQ (Jégou et al., 2011) and RabitQ (Gao &amp; Long, 2024) save space but lose accuracy under tight budgets. In contrast, LEANN combines on-the-fly embedding recomputation with a pruned graph index and optimized traversal for personal devices.\n\nVector search applications on personal devices. On-device vector search enables privacy-preserving, low-latency, and offline capabilities across diverse applications. On-device RAG systems ground language models in personal document collections while maintaining data privacy (Ryan et al., 2024; Wang &amp; Chau, 2024; Lee et al., 2024; Zerhoudi &amp; Granitzer, 2024). Personalized recommenders (Yin et al., 2024) match user profiles with item embeddings on the device, while vision-based search (Ren et al., 2023) retrieves local images or videos to assist downstream QA or generation tasks. These applications motivate the design of LEANN to enable efficient, low-overhead\n\nvector search on personal devices.\n"
  },
  {
    "title": "9 Conclusions",
    "node_id": "0011",
    "summary": "# 9 Conclusions\n\nSimilarity search over high-dimensional embeddings underpins many generative AI applications such as RAG. However, enabling such capabilities remains challenging due to the substantial storage required for embeddings and index metadata. We present LEANN, a storage-efficient vector index based on graph-based recomputation. By combining two-level search with dynamic batching, LEANN supports efficient query processing without storing the full embedding set. A high-degree preserving pruning strategy further reduces graph storage while maintaining accuracy. LEANN also offers fast, storage-efficient index construction and update pipelines. Together, these techniques allow LEANN to operate with an index smaller than 5% of the raw data size, achieving up to 50$\\times$ storage reduction compared to existing methods while preserving high recall and low latency.\n",
    "start_index": 11,
    "end_index": 11,
    "text": "# 9 Conclusions\n\nSimilarity search over high-dimensional embeddings underpins many generative AI applications such as RAG. However, enabling such capabilities remains challenging due to the substantial storage required for embeddings and index metadata. We present LEANN, a storage-efficient vector index based on graph-based recomputation. By combining two-level search with dynamic batching, LEANN supports efficient query processing without storing the full embedding set. A high-degree preserving pruning strategy further reduces graph storage while maintaining accuracy. LEANN also offers fast, storage-efficient index construction and update pipelines. Together, these techniques allow LEANN to operate with an index smaller than 5% of the raw data size, achieving up to 50$\\times$ storage reduction compared to existing methods while preserving high recall and low latency.\n",
    "nodes": [
      {
        "title": "References",
        "node_id": "0012",
        "summary": "The provided text is a list of references covering a wide range of topics in artificial intelligence, machine learning, and computer science. Key areas include retrieval-augmented generation (RAG) systems, approximate nearest neighbor (ANN) search algorithms and indexing techniques (graph-based, product quantization, hierarchical navigable small world graphs), benchmarking datasets and tools (MS MARCO, TriviaQA, Natural Questions, FinanceBench, GPQA), large language models (LLMs) and their technical reports (Qwen, RedPajama), multimodal embeddings, speech recognition, private web search, on-device AI solutions (edge devices, mobile deployments), vector databases and search engines, and related hardware (AWS EC2, NVIDIA GPUs). The references also touch upon data compression, privacy, and system trade-offs in AI inference.",
        "start_index": 11,
        "end_index": 14,
        "text": "## References\n\n- Asai et al. [2023] Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In *The Twelfth International Conference on Learning Representations*, 2023.\n- Aumüller et al. [2020] Aumüller, M., Bernhardsson, E., and Faithfull, A. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. *Information Systems*, 87:101374, 2020.\n- AWS [2023] AWS. Amazon EC2 G5 instance. https://aws.amazon.com/ec2/instance-types/mac/, 2023. [Online; accessed April-2025].\n- Bai et al. [2025] Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. *arXiv preprint arXiv:2502.13923*, 2025.\n- Cai et al. [2024] Cai, D., Wang, S., Peng, C., et al. Recall: Empowering multimodal embedding for edge devices. arXiv:2409.15342, 2024.\n- Computer et al. [2023] Computer, T. RedPajama: An open source recipe to reproduce LLaMA training dataset. https://github.com/togethercomputer/RedPajama-Data, 2023. Accessed: May 10, 2025.\n- Craswell et al. [2021] Craswell, N., Mitra, B., Yilmaz, E., Campos, D., and Lin, J. Ms marco: Benchmarking ranking models in the large-data regime. In *proceedings of the 44th International ACM SIGIR conference on research and development in information retrieval*, pp. 1566–1576, 2021.\n- Douze et al. [2021] Douze, M., Sablayrolles, A., and Jégou, H. Link and code: Fast indexing with graphs and compact regression codes. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 3646–3654, 2018.\n- Douze et al. [2024] Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.-E., Lomeli, M., Hosseini, L., and Jégou, H. The faiss library, 2025. URL https://arxiv.org/abs/2401.08281.\n- Fu et al. [2019] Fu, C., Xiang, C., Wang, C., and Cai, D. Fast approximate nearest neighbor search with the navigating spreading-out graph. *Proc. VLDB Endow.*, 12(5):461–474, January 2019. ISSN 2150-8097. doi: 10.14778/3303753.3303754. URL https://doi.org/10.14778/3303753.3303754.\n- Gao et al. [2024] Gao, J. and Long, C. RabitQ: Quantizing high-dimensional vectors with a theoretical error bound for approximate nearest neighbor search. In *Proceedings of the ACM on Management of Data (SIGMOD ’24)*, volume 2, 2024.\n- He et al. [2019] He, Y., Sainath, T. N., Prabhavalkar, R., McGraw, I., Alvarez, R., Zhao, D., et al. Streaming end-to-end speech recognition for mobile devices. In *Proc. IEEE ICASSP*, pp. 6381–6385, 2019.\n- Henzinger et al. [2023] Henzinger, A., Dauterman, E., Corrigan-Gibbs, H., and Zeldovich, N. Private web search with tiptoe. Cryptology ePrint Archive, Paper 2023/1438, 2023. URL https://eprint.iacr.org/2023/1438.\n- Islam et al. [2021] Islam, P., Kannappan, A., Kiela, D., Qian, R., Scherrer, N., and Vidgen, B. Financebench: A new benchmark for financial question answering, 2023. URL https://arxiv.org/abs/2311.11944.\n- Izacard et al. [2021] Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. *arXiv preprint arXiv:2112.09118*, 2021.\n- Jaromczyk and Toussaint [2011] Jaromczyk, J. and Toussaint, G. Relative neighborhood graphs and their relatives. *Proceedings of the IEEE*, 80(9):1502–1517, 1992. doi: 10.1109/5.163414.\n- Joshi et al. [2011] Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. *arXiv preprint arXiv:1705.03551*, 2017.\n- Jégou et al. [2011] Jégou, H., Douze, M., and Schmid, C. Product quantization for nearest neighbor search. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 33(1):117–128, 2011. doi: 10.1109/TPAMI.2010.57.\n- Kwiatkowski et al. [2010] Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and\n\nPetrov, S. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026/.\n- Lee et al. [2019] Lee, C., Prahlad, D., Kim, D., and Kim, H. Work-in-progress: On-device retrieval augmented generation with knowledge graphs for personalized large language models. In 2024 International Conference on Embedded Software (EMSOFT), pp. 1–1, 2024. doi: 10.1109/EMSOFT60242.2024.00006.\n- Lempitsky et al. [2012] Lempitsky, V. The inverted multi-index. In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR ’12, pp. 3069–3076, USA, 2012. IEEE Computer Society. ISBN 9781467312264.\n- Li et al. [2023] Li, Z., Zhang, X., Zhang, Y., Long, D., Xie, P., and Zhang, M. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023.\n- Lin et al. [2021] Lin, J., Ma, X., Lin, S.-C., Yang, J.-H., Pradeep, R., and Nogueira, R. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021), pp. 2356–2362, 2021.\n- Mageirakos et al. [2025] Mageirakos, V., Wu, B., and Alonso, G. Cracking vector search indexes. arXiv preprint arXiv:2503.01823, 2025.\n- Malkov and Yashunin [2021] Malkov, Y. A. and Yashunin, D. A. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824–836, 2018.\n- Manohar et al. [2023] Manohar, M. D., Shen, Z., Blelloch, G., Dhulipala, L., Gu, Y., Simhadri, H. V., and Sun, Y. Parlayann: Scalable and deterministic parallel graph-based approximate nearest neighbor search algorithms. In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, pp. 270–285, 2024.\n- Mohoney et al. [2024] Mohoney, J., Pacaci, A., Chowdhury, S. R., Mousavi, A., Ilyas, I. F., Minhas, U. F., Pound, J., and Rekatsinas, T. High-throughput vector similarity search in knowledge graphs. Proceedings of the ACM on Management of Data, 1(2):1–25, 2023.\n- Munyampirwa et al. [2023] Munyampirwa, B., Lakshman, V., and Coleman, B. Down with the hierarchy: The’h’in hnsw stands for” hubs”. arXiv preprint arXiv:2412.01940, 2024.\n- NVIDIA [2025] NVIDIA geforce rtx 4090 graphics card. https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/, 2022. Accessed: 2025-08-20.\n- ObjectBox [2025] ObjectBox. On-device vector search. https://docs.objectbox.io/on-device-vector-search, 2025. Accessed: May 15, 2025.\n- Pan et al. [2023] Pan, Y., Sun, J., and Yu, H. Lm-diskann: Low memory footprint in disk-native dynamic graph-based ann indexing. In 2023 IEEE International Conference on Big Data (BigData), pp. 5987–5996, 2023. doi: 10.1109/BigData59044.2023.10386517.\n- Rein et al. [2023] Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024.\n- Rekabsaz et al. [2024] Rekabsaz, N., Lesota, O., Schedl, M., Brassey, J., and Eickhoff, C. Tripclick: the log files of a large health web search engine. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2507–2513, 2021.\n- Ren et al. [2020] Ren, J., Zhang, M., and Li, D. Hm-ann: efficient billion-point nearest neighbor search on heterogeneous memory. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n- Ren et al. [2025] Ren, J., Tulyakov, S., Peng, K.-C., Wang, Z., and Shi, H. Efficient neural networks: From algorithm design to practical mobile deployments. CVPR 2023 Tutorial, 2023. https://snap-research.github.io/efficient-nn-tutorial/.\n- Research [2025] Research, F. A. Guidelines to choose an index. https://github.com/facebookresearch/fais/s/wiki/Guidelines-to-choose-an-index/28074dc0ddc733f84b06fa4d99b3f6e2ef65613d#if-below-lm-vectors-ivfx, 2025. Accessed: 2025-05-10.\n- Ryan et al. [2024] Ryan, M. J., Xu, D., Nivera, C., and Campos, D. EnronQA: Towards personalized RAG over private documents. arXiv preprint arXiv:2505.00263, 2024.\n- Schuhmann et al. [2024] Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n- Seemakhupt et al. [2024] Seemakhupt, K., Liu, S., and Khan, S. Edgerag: Online-indexed rag for edge devices. arXiv preprint arXiv:2412.21023, 2024.\n\nSevero, D., Ottaviano, G., Muckley, M., Ullrich, K., and Douze, M. Lossless compression of vector ids for approximate nearest neighbor search. arXiv preprint arXiv:2501.10479, 2025.\n- Shao et al. [2024] Shao, R., He, J., Asai, A., Shi, W., Dettmers, T., Min, S., Zettlemoyer, L., and Koh, P. W. W. Scaling retrieval-based language models with a trillion-token datastore. Advances in Neural Information Processing Systems, 37: 91260–91299, 2024.\n- Shen et al. [2024] Shen, M., Umar, M., Maeng, K., Suh, G. E., and Gupta, U. Towards understanding systems trade-offs in retrieval-augmented generation model inference, 2024. URL https://arxiv.org/abs/2412.11854.\n- Subramanya et al. [2019] Subramanya, S. J., Devvrit, Kadekodi, R., Krishaswamy, R., and Simhadri, H. V. DiskANN: fast accurate billion-point nearest neighbor search on a single node. Curran Associates Inc., Red Hook, NY, USA, 2019.\n- Tatsuno et al. [2024] Tatsuno, K., Miyashita, D., Ikeda, T., Ishiyama, K., Sumiyoshi, K., and Deguchi, J. AiSAQ: all-in-storage ANNS with product quantization for DRAM-free information retrieval. arXiv preprint arXiv:2404.06004, 2024. URL https://arxiv.org/abs/2404.06004.\n- Tian et al. [2021] Tian, B., Liu, H., Tang, Y., Xiao, S., Duan, Z., Liao, X., Jin, H., Zhang, X., Zhu, J., and Zhang, Y. Towards high-throughput and low-latency billion-scale vector search via CPU/GPU collaborative filtering and re-ranking. In 23rd USENIX Conference on File and Storage Technologies (FAST 25), pp. 171–185, Santa Clara, CA, February 2025. USENIX Association. ISBN 978-1-939133-45-8. URL https://www.usenix.org/conference/fast25/presentation/tian-bing.\n- Toussaint et al. [2024] Toussaint, G. T. The relative neighbourhood graph of a finite planar set. Pattern Recognit., 12:261–268, 1980. URL https://api.semanticscholar.org/CorpusID:2830642.\n- Wang et al. [2021] Wang, M., Xu, X., Yue, Q., and Wang, Y. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. arXiv preprint arXiv:2101.12631, 2021.\n- Wang et al. [2024] Wang, M., Xu, W., Yi, X., Wu, S., Peng, Z., Ke, X., Gao, Y., Xu, X., Guo, R., and Xie, C. Starling: An i/o-efficient disk-resident graph index framework for high-dimensional vector similarity search on data segment. In Proceedings of the ACM on Management of Data (SIGMOD ’24), volume 2, 2024. doi: 10.1145/3639269.3652200.\n- Wang and Chau [2024] Wang, Z. J. and Chau, D. H. Mememo: On-device retrieval augmentation for private and personalized text generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2765–2770, 2024.\n- Yang et al. [2024] Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\n- Yang et al. [2025] Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.\n- Yin et al. [2024] Yin, H., Chen, T., Qu, L., and Cui, B. On-device recommender systems: A comprehensive survey. arXiv preprint arXiv:2401.11441, 2024.\n- Zerhoudi et al. [2024] Zerhoudi, S. and Granitzer, M. Personarag: Enhancing retrieval-augmented generation systems with user-centric agents. arXiv preprint arXiv:2407.09394, 2024.\n- Zhang et al. [2024] Zhang, Z., Li, Z., Zhang, Z., and Yu, P. S. MicroNN: An embedded vector search engine for low-resource environments. arXiv preprint arXiv:2504.05573, 2025.\n- Zhu et al. [2024] Zhu, J., Patel, L., Zaharia, M., and Popa, R. A. Compass: Encrypted semantic search with high accuracy. Cryptology ePrint Archive, Paper 2024/1255, 2024. URL https://eprint.iacr.org/2024/1255.\n"
      }
    ]
  },
  {
    "title": "A RNG PRUNING",
    "node_id": "0013",
    "summary": "The text describes the Random Geometric Graph (RNG) pruning technique used in proximity-graph indexes like HNSW. When a node 'v' is inserted, candidate neighbors are found and sorted by distance. The RNG rule then prunes a candidate 'x' if a closer neighbor 'a' exists such that the distance from 'a' to 'x' is less than the distance from 'v' to 'x'. This effectively removes the longest edge in the triangle formed by 'v', 'a', and 'x', leading to sparse graphs commonly used in modern Approximate Nearest Neighbor (ANN) indexes.",
    "start_index": 14,
    "end_index": 14,
    "text": "# A RNG PRUNING\n\nFor a node  $v$  inserted into any proximity-graph index (including HNSW), the algorithm first searches for a list of candidate neighbors  $a, b, c, d$  (see Algorithm 1) sorted by distance from  $v$ . The RNG-based pruning rule(Jaromczyk &amp; Toussaint, 1992; Toussaint, 1980), implemented in Line 6, then iterates through this list in order of increasing distance. A candidate  $x$  is pruned if there exists a closer neighbor  $a$  such that  $\\mathrm{Dist}(a, x) &lt; \\mathrm{Dist}(v, x)$ . This effectively removes the longest edge in the triangle formed by  $(v, a, x)$ . As illustrated in Figure 9, the edges  $v - b$  and  $v - c$  are pruned, so the search from  $v$  to  $b$  or  $c$  proceeds indirectly through  $a$ . This pruning strategy is widely used in modern graph-based ANN indexes and makes the resulting graph extremely sparse (Munyampirwa et al., 2024; Malkov &amp; Yashunin, 2018).\n\n![img-9.jpeg](img-9.jpeg)\nFigure 9. Select neighbors from candidate nodes using RNG.\n"
  },
  {
    "title": "B LEANN UPDATE STRATEGY",
    "node_id": "0014",
    "summary": "# B LEANN UPDATE STRATEGY\n\nThe ADD algorithm is presented in Algorithm 4.\n",
    "start_index": 14,
    "end_index": 14,
    "text": "# B LEANN UPDATE STRATEGY\n\nThe ADD algorithm is presented in Algorithm 4.\n",
    "nodes": [
      {
        "title": "B.1 Add Operation: Method and Time Complexity",
        "node_id": "0015",
        "summary": "The text details the time complexity of the ADD operation in LEANN, starting with a naive implementation and its associated costs for neighbor list shrinking and edge additions. It then introduces caching as an optimization to reduce redundant computations, followed by a simplified RNG pruning approach. Finally, it presents the optimized complexity achieved through these simplifications, highlighting the reduction from cubic to linear in terms of maximum node degree (M) while preserving graph connectivity.",
        "start_index": 14,
        "end_index": 14,
        "text": "## B.1 Add Operation: Method and Time Complexity\n\nNaive Implementation. A naive implementation of the ADD operation in LEANN recomputes all distances from scratch since only the graph structure is stored. Its total time complexity can be expressed as:\n\n$$\nO (M \\cdot e f C + e f C ^ {2} + M ^ {3}),\n$$\n\nwhere  $efC$  is the construction queue length and  $M$  the maximum node degree.\n\nWe first analyze the complexity of SHRINKNEIGHBORLIST. Each node placed in the retained set  $R$  may be re-examined up to  $|W|$  times, giving a cost of  $O(|W|^2)$  for a single call.\n\nSpecifically:\n\n- SEARCHNEIGHBORSTOADD performs a one-time neighbor search without revisiting nodes, yielding  $O(M \\cdot efC)$ . Caching offers no benefit since nodes are not revisited.\n- SHRINKNEIGHBORLIST (Line 12) runs in  $O(efC^2)$ , as it computes pairwise distances among up to  $efC$  candidates.\n\n- Adding forward edges requires no recomputation, since each node maintains at most  $M$  links.\n- Adding reverse edges costs  $O(M^3)$ , as up to  $M$  neighbors are updated and each triggers an  $O(M^2)$  RNG-based shrink.\n\nCaching Optimization. To improve efficiency, LEANN introduces a distance cache to eliminate redundant computations in the SHRINK step, reducing the overall complexity to:\n\n$$\nO (M \\cdot e f C + e f C + M ^ {2}),\n$$\n\nsince the shrink operation now costs only  $O(|W|)$ .\n\nSimplified RNG Pruning. By further simplifying SHRINKNEIGHBORLIST to randomly select neighbors instead of performing full RNG checks, the complexity becomes:\n\n$$\nO (M \\cdot e f C + M ^ {2}).\n$$\n\nFinally, applying the same simplification to the reverse-edge update step yields the optimized complexity:\n\n$$\nO (M \\cdot e f C),\n$$\n\nreducing the cost from cubic to linear in  $M$  while maintaining comparable graph connectivity.\n"
      },
      {
        "title": "B.2 Batched Add Operation: Optimization",
        "node_id": "0016",
        "summary": "## B.2 Batched Add Operation: Optimization\n\nWhen a batch of add operations is followed by a search request, LEANN does not immediately insert all new passages. Instead, it temporarily buffers their embeddings and merges search results from both the existing graph and the buffered embeddings. After the search completes, the buffered passages are inserted asynchronously, a process we term delayed insertion.\n\nThe same system optimizations described earlier can be reused here, with the addition of a global cache to avoid redundant computations across multiple add requests. To maintain storage efficiency, LEANN monitors the cache size and clears it once a predefined budget is reached, starting a new round of batched insertion.\n",
        "start_index": 14,
        "end_index": 14,
        "text": "## B.2 Batched Add Operation: Optimization\n\nWhen a batch of add operations is followed by a search request, LEANN does not immediately insert all new passages. Instead, it temporarily buffers their embeddings and merges search results from both the existing graph and the buffered embeddings. After the search completes, the buffered passages are inserted asynchronously, a process we term delayed insertion.\n\nThe same system optimizations described earlier can be reused here, with the addition of a global cache to avoid redundant computations across multiple add requests. To maintain storage efficiency, LEANN monitors the cache size and clears it once a predefined budget is reached, starting a new round of batched insertion.\n"
      },
      {
        "title": "B.3 Soft Deletion Strategy",
        "node_id": "0017",
        "summary": "The text details LEANN's soft deletion strategy for graph nodes, which uses a binary flag for O(1) updates and filters deleted nodes during queries. It also outlines Algorithm 4 for node insertion into the graph index, involving neighbor search, list shrinking, and edge additions, with a mechanism to manage node degrees. Appendix C notes evaluation details.",
        "start_index": 14,
        "end_index": 15,
        "text": "## B.3 Soft Deletion Strategy\n\nLEANN adopts a simple soft delete for graph nodes. Each node keeps a binary delete flag, so removal is an  $O(1)$  update that leaves the adjacency list untouched. During query processing, we still traverse deleted nodes to reach their neighbors, but before producing results we filter the candidate queue (the  $EQ$  in Algorithm 2) by this flag and then take the top- $k$  active entries to guarantee correctness.\n\nIf the fraction of deleted nodes grows beyond a threshold (e.g.,  $5\\%$ ), we can trigger a background rebuild. Exploring such policies is left for future work.\n\nAlgorithm 4 Node Insertion into Graph Index\n1: Input: Existing graph $G$; construction queue length $efC$; maximum degree $M$; node to insert $v$\n2: Output: Updated graph $G$ including node $v$\n3: function SHRINK($W,M$)\n4: Initialize $R\\leftarrow\\emptyset$\n5: for $x$ in $W$ (by ascending distance to query $q$) do\n6: if no $y\\in R$ s.t. Dist$(x,y)<$ Dist$(x,q)$ then\n7: Add $x$ to $R$\n8: if $|R|=M$ then\n9: break\n10: return $R$\n11: $W\\leftarrow$ SEARCH($v,efC$) $\\triangleright$ See Algorithm 1\n12: $W\\leftarrow$ SHRINK($W,M$) $\\triangleright$ ShrinkNeighborList\n13: Add directed edges from $v$ to all nodes in $W$\n14: for each $w\\in W$ do\n15: Add directed edge from $w$ to $v$\n16: if deg$(w)>M$ then\n17: SHRINK($w$’s neighbor list, $M$)\n\n### Appendix C Evaluation details\n"
      },
      {
        "title": "C.1 Baseline Configurations",
        "node_id": "0018",
        "summary": "The text describes baseline configurations for various indexing and retrieval methods. These include HNSW (using faiss.IndexHNSWFlat with specific parameters), IVF (using faiss.IndexIVFFlat with a calculated number of centroids), DiskANN (with specified parameters and disk-based embedding loading), IVF-Disk (employing memory-mapped files for reduced memory footprint), IVF-Recompute (recomputing embeddings at query time), PQ Compression (compressing vectors to a specified size), and BM25 (a classical lexical ranking method implemented via Pyserini).",
        "start_index": 15,
        "end_index": 15,
        "text": "## C.1 Baseline Configurations\n\n- HNSW *(Malkov and Yashunin, 2018)*: We use the faiss.IndexHNSWFlat implementation with construction parameters recommended by FAISS: $M{=}30$ and $efConstruction{=}128$, distinct from the search-time parameter $ef$.\n- IVF *(Lempitsky, 2012)*: We adopt the faiss.IndexIVFFlat implementation. Following best practices from FAISS *(Research, 2025)* and prior work *(Henzinger et al., 2023)*, we set the number of centroids to $\\sqrt{N}$, where $N$ is the size of the datastore. For our $N{=}60$M setup, this corresponds to $nlist{=}8192$.\n- DiskANN *(Subramanya et al., 2019)*: We use DiskANN with $M{=}60$ and $efConstruction{=}128$, following recommended settings *(Subramanya et al., 2019)*. It stores only a PQ table in memory and loads full embeddings from disk on demand.\n- IVF-Disk: Reduces memory usage by employing memory-mapped files (mmap) instead of loading the entire index into memory. Implemented using FAISS’s faiss.contrib.ondisk module with the same parameters as IVF.\n- IVF-Recompute *(Seemakhupt et al., 2024)*: Inspired by Edge-RAG, this variant recomputes embeddings at query time instead of storing them, using the same construction parameters as IVF.\n- PQ Compression *(Jégou et al., 2011)*: Applies Product Quantization to compress stored embeddings while preserving the graph structure. For fair comparison, we compress the vectors to 5 GB—slightly larger than our system’s 4 GB footprint. We use the PQ implementation from *(Subramanya et al., 2019)*.\n- BM25 *(Craswell et al., 2021; Rekabsaz et al., 2021)*: A classical lexical ranking method widely used in keyword-based retrieval systems. We employ the standard implementation from Pyserini *(Lin et al., 2021)*.\n"
      },
      {
        "title": "C.2 Latency Measurement and Evaluation Protocol",
        "node_id": "0019",
        "summary": "The text describes a protocol for evaluating retrieval accuracy and latency. Retrieval accuracy is measured using Recall@3, with exact search results serving as ground truth. Latency is evaluated by performing a binary search to find the minimal search queue length (ef) required to achieve target recall levels, and then recording the average retrieval latency over 20 random queries.",
        "start_index": 15,
        "end_index": 15,
        "text": "## C.2 Latency Measurement and Evaluation Protocol\n\nTo evaluate retrieval accuracy, we report Recall@k as defined in §2. In open-domain settings, ground-truth labels for retrieved passages are typically unavailable. Following standard practice *(Jégou et al., 2011; Schuhmann et al., 2021; Zhu et al., 2024)*, we treat the results from exact search as a proxy for ground truth. In all experiments, we set $k{=}3$, consistent with prior work *(Shao et al., 2024; Asai et al., 2023)*, and report Recall@3 as our retrival accuracy metric.\n\nFor latency evaluation, we measure the time required to achieve different target recall levels. Specifically, we perform a binary search to find the minimal search queue length $ef$ (as defined in Algorithm 1) that reaches the desired recall. Using the resulting $ef$, we record the average retrieval latency over 20 random queries.\n"
      },
      {
        "title": "C.3 Latency Measurement in RAG Pipeline",
        "node_id": "0020",
        "summary": "## C.3 Latency Measurement in RAG Pipeline\n\nWe evaluate the latency of LEANN at the 90% recall level across all datasets. For text generation, we use Qwen3-4B *(Yang et al., 2025)*, and for multimodal workloads, we use Qwen2.5-VL-7B-Instruct *(Bai et al., 2025)*. Both the embedding and generation models are implemented using the Hugging Face framework.\n",
        "start_index": 15,
        "end_index": 15,
        "text": "## C.3 Latency Measurement in RAG Pipeline\n\nWe evaluate the latency of LEANN at the 90% recall level across all datasets. For text generation, we use Qwen3-4B *(Yang et al., 2025)*, and for multimodal workloads, we use Qwen2.5-VL-7B-Instruct *(Bai et al., 2025)*. Both the embedding and generation models are implemented using the Hugging Face framework.\n"
      },
      {
        "title": "C.4 RAG Latency on Mac Platform",
        "node_id": "0021",
        "summary": "## C.4 RAG Latency on Mac Platform\n\nTo validate the generalizability of our results across different hardware platforms, we conducted additional experiments on Mac hardware. Table 4 presents the vector search and end-to-end RAG latency measurements on Mac, following the same experimental protocol as the RTX 4090 results shown in the main paper. The results demonstrate that LEANN maintains its efficiency advantages on Mac hardware, with retrieval overhead remaining low compared to other methods despite the different underlying architecture and computational characteristics.\n\n### Appendix D More Ablation Studies\n",
        "start_index": 15,
        "end_index": 15,
        "text": "## C.4 RAG Latency on Mac Platform\n\nTo validate the generalizability of our results across different hardware platforms, we conducted additional experiments on Mac hardware. Table 4 presents the vector search and end-to-end RAG latency measurements on Mac, following the same experimental protocol as the RTX 4090 results shown in the main paper. The results demonstrate that LEANN maintains its efficiency advantages on Mac hardware, with retrieval overhead remaining low compared to other methods despite the different underlying architecture and computational characteristics.\n\n### Appendix D More Ablation Studies\n"
      },
      {
        "title": "D.1 Comparison of Index Construction",
        "node_id": "0022",
        "summary": "This text compares storage-efficient index construction techniques against standard HNSW, evaluating two sharded merging pipeline variants: k-means-based and random assignment. The k-means approach, after partitioning data into 15 shards and achieving a 5x storage reduction, maintains high recall with minimal recomputation increase, validating the benefit of pre-sharding clustering. In contrast, random sharding requires more recomputation for similar recall. Table 4 provides latency and overhead data for various methods (DiskANN, IVF-Disk, IVF-Recompute, LEANN) across different datasets on a Mac, noting that HNSW, IVF, PQ, and BM25 were omitted due to OOM errors or insufficient accuracy.",
        "start_index": 15,
        "end_index": 16,
        "text": "## D.1 Comparison of Index Construction\n\nWe evaluate the storage-efficient index construction techniques introduced in §6, comparing them against the standard HNSW construction. Specifically, we implement two\n\nTable 4. Vector search and end-to-end RAG latency across datasets on Mac. Latency is reported at  $90\\%$  recall. Results for PQ and BM25 are omitted as they fail to reach this accuracy (their downstream accuracy is also low in Figure 4). Results for HNSW and IVF are omitted, as they cause OOM on the local Mac and on all EC2 Mac instances on AWS. Overhead (\\%) denotes the ratio of retrieval latency to total pipeline latency (retrieval / [retrieval + generation]).\n\n|  Dataset | Generation (s) | Method | Retrieval (s) | Overhead (%) | Dataset | Generation (s) | Method | Retrieval (s) | Overhead (%)  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  NQ | 45.42 | HNSW | - | - | GPQA | 132.24 | HNSW | - | -  |\n|   |   |  IVF | - | - |   |   | IVF | - | -  |\n|   |   |  DiskANN | 0.37 | 0.8 |   |   | DiskANN | 0.29 | 0.2  |\n|   |   |  IVF-Disk | 2.94 | 6.1 |   |   | IVF-Disk | 0.11 | 0.1  |\n|   |   |  IVF-Recompute | 2446.60 | 98.2 |   |   | IVF-Recompute | 174.06 | 56.8  |\n|   |   |  PQ Compression | - | - |   |   | PQ Compression | - | -  |\n|   |   |  BM25 | - | - |   |   | BM25 | - | -  |\n|   |   |  LEANN | 13.84 | 23.4 |   |   | LEANN | 5.22 | 3.8  |\n|  TriviaQA | 52.92 | HNSW | - | - | HotpotQA | 44.67 | HNSW | - | -  |\n|   |   |  IVF | - | - |   |   | IVF | - | -  |\n|   |   |  DiskANN | 0.98 | 1.8 |   |   | DiskANN | 1.91 | 4.1  |\n|   |   |  IVF-Disk | 2.64 | 4.8 |   |   | IVF-Disk | 3.54 | 7.4  |\n|   |   |  IVF-Recompute | 3174.41 | 98.4 |   |   | IVF-Recompute | 3415.74 | 98.7  |\n|   |   |  PQ Compression | - | - |   |   | PQ Compression | - | -  |\n|   |   |  BM25 | - | - |   |   | BM25 | - | -  |\n|   |   |  LEANN | 17.15 | 24.5 |   |   | LEANN | 44.80 | 50.1  |\n\n![img-10.jpeg](img-10.jpeg)\nFigure 10. [Ablation Study]: Comparison of storage-efficient index construction methods with the original HNSW.\n\nvariants of the proposed sharded merging pipeline approach: (1) a  $k$ -means-based method (see §6) that groups similar passages before sharding, and (2) a random assignment baseline that omits clustering. We assess graph quality using the same methodology as before, with results shown in Figure 10. In this setup, the dataset is partitioned into 15 shards, achieving about a  $5 \\times$  reduction in peak storage usage during index construction.\n\nThe  $k$ -means-sharded graph achieves nearly the same recall as the original HNSW with only a small increase in recomputation cost, indicating that it maintains strong connectivity after sharding and merging. In contrast, the randomly sharded graph requires much more recomputation to reach the same recall. These results highlight the benefit of clustering similar passages before sharding, validating the design of our sharded merging pipeline approach.\n"
      },
      {
        "title": "D.2 Using Different Embedding Model Sizes",
        "node_id": "0023",
        "summary": "This section explores reducing system latency by using a smaller embedding model, specifically replacing the Contriever model with the GTE-small model. Evaluations on a 2M-chunk datastore with ef = 50 show that GTE-small achieves a 2.3x speedup while maintaining downstream accuracy within 2% of the baseline, demonstrating that LEANN can utilize lighter encoders to decrease latency without compromising answer quality.",
        "start_index": 16,
        "end_index": 16,
        "text": "## D.2 Using Different Embedding Model Sizes\n\nSince the primary bottleneck of our system lies in the recomputation process, we further explore the potential for latency reduction by adopting a smaller embedding model. Specif\n\n![img-11.jpeg](img-11.jpeg)\n(a) Accuracy\n\n![img-12.jpeg](img-12.jpeg)\n(b) Latency\nFigure 11. [Ablation Study]: Downstream accuracy and end-to-end latency when swapping the embedding model for a lightweight alternative on a 2M-chunk datastore with  $\\mathrm{ef} = 50$ .\n\nically, we replace the original Contriever model (110M parameters) used in §7.1 with the lightweight GTE-small model (Li et al., 2023), which has only 34M parameters. We evaluate performance on a 2M document datastore using a fixed search queue length of  $\\mathrm{ef} = 50$ . As shown in Figure 11, GTE-small achieves a  $2.3 \\times$  speedup while maintaining downstream task accuracy within  $2\\%$  of the Contriever baseline, demonstrating that LEANN can further reduce latency by leveraging lighter encoders without sacrificing answer quality.\n"
      },
      {
        "title": "D.3 Relaxing Disk Constraint",
        "node_id": "0024",
        "summary": "This text discusses the benefits of relaxing disk constraints in LEANN, enabling the materialization of high-degree node embeddings to reduce recomputation overhead. An ablation study (Figure 12) demonstrates significant latency improvements and cache-hit rates across various datasets and storage budgets, with storing just 10% of embeddings yielding a 1.5x speedup and up to 41.9% cache hit rate. The high cache hit rate is linked to skewed access patterns, though SSD loading overhead impacts the exact latency gains. Figure 13 further details the latency breakdown during graph-based recomputation.",
        "start_index": 16,
        "end_index": 17,
        "text": "## D.3 Relaxing Disk Constraint\n\nWhen disk storage constraints are relaxed, LEANN can materialize the embeddings of high-degree nodes to reduce recomputation overhead. Figure 12 quantifies the resulting latency improvements and cache-hit rates across four datasets while varying the fraction of cached embeddings. Storing just  $10\\%$  of the original embeddings yields a  $1.5 \\times$  speedup, with a cache hit rate of up to  $41.9\\%$ . This high\n\n![img-13.jpeg](img-13.jpeg)\nFigure 12. [Ablation Study]: Latency and cache-hit rate comparison under varying storage budgets.\n\n![img-14.jpeg](img-14.jpeg)\nFigure 13. [Ablation Study]: Latency breakdown of a batch of requests during graph-based recomputation.\n\ncache hit rate arises from the skewed access pattern characteristic of graph-based traversal, though SSD loading overhead prevents the latency gains from matching the hit rate exactly.\n"
      },
      {
        "title": "D.4 Graph-based Recomputation Breakdown",
        "node_id": "0025",
        "summary": "## D.4 Graph-based Recomputation Breakdown\n\nFigure 13 decomposes the latency of a batched query into three stages: PQ lookup, text processing, and embedding recomputation. Each batch aggregates multiple hops of recomputation, as described in §4.2. First, LEANN performs PQ lookups to select promising nodes, then retrieves and tokenizes the corresponding raw text. The tokenized inputs are sent to the local embedding generator. Finally, LEANN performs embedding recomputation and distance calculation. Although embedding recomputation is the primary bottleneck in LEANN, accounting for roughly  $76\\%$  of total latency, the three stages span I/O, CPU, and GPU resources, indicating opportunities to overlap work and further improve efficiency.\n",
        "start_index": 17,
        "end_index": 17,
        "text": "## D.4 Graph-based Recomputation Breakdown\n\nFigure 13 decomposes the latency of a batched query into three stages: PQ lookup, text processing, and embedding recomputation. Each batch aggregates multiple hops of recomputation, as described in §4.2. First, LEANN performs PQ lookups to select promising nodes, then retrieves and tokenizes the corresponding raw text. The tokenized inputs are sent to the local embedding generator. Finally, LEANN performs embedding recomputation and distance calculation. Although embedding recomputation is the primary bottleneck in LEANN, accounting for roughly  $76\\%$  of total latency, the three stages span I/O, CPU, and GPU resources, indicating opportunities to overlap work and further improve efficiency.\n"
      }
    ]
  }
]